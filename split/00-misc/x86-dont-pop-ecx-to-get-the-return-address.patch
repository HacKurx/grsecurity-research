diff -NurpX linux-4.9.24-pax/Documentation/dontdiff linux-4.9.24/arch/x86/boot/compressed/efi_stub_32.S linux-4.9.24-pax/arch/x86/boot/compressed/efi_stub_32.S
--- linux-4.9.24/arch/x86/boot/compressed/efi_stub_32.S	2015-03-18 15:21:50.224349253 +0100
+++ linux-4.9.24-pax/arch/x86/boot/compressed/efi_stub_32.S	2017-01-01 22:57:10.597520716 +0100
@@ -46,16 +46,13 @@ ENTRY(efi_call_phys)
 	 * parameter 2, ..., param n. To make things easy, we save the return
 	 * address of efi_call_phys in a global variable.
 	 */
-	popl	%ecx
-	movl	%ecx, saved_return_addr(%edx)
-	/* get the function pointer into ECX*/
-	popl	%ecx
-	movl	%ecx, efi_rt_function_ptr(%edx)
+	popl	saved_return_addr(%edx)
+	popl	efi_rt_function_ptr(%edx)
 
 	/*
 	 * 3. Call the physical function.
 	 */
-	call	*%ecx
+	call	*efi_rt_function_ptr(%edx)
 
 	/*
 	 * 4. Balance the stack. And because EAX contain the return value,
@@ -67,15 +64,12 @@ ENTRY(efi_call_phys)
 1:	popl	%edx
 	subl	$1b, %edx
 
-	movl	efi_rt_function_ptr(%edx), %ecx
-	pushl	%ecx
+	pushl	efi_rt_function_ptr(%edx)
 
 	/*
 	 * 10. Push the saved return address onto the stack and return.
 	 */
-	movl	saved_return_addr(%edx), %ecx
-	pushl	%ecx
-	ret
+	jmpl	*saved_return_addr(%edx)
 ENDPROC(efi_call_phys)
 .previous
 
diff -NurpX linux-4.9.24-pax/Documentation/dontdiff linux-4.9.24/arch/x86/crypto/crc32-pclmul_asm.S linux-4.9.24-pax/arch/x86/crypto/crc32-pclmul_asm.S
--- linux-4.9.24/arch/x86/crypto/crc32-pclmul_asm.S	2013-07-08 01:58:01.360266200 +0200
+++ linux-4.9.24-pax/arch/x86/crypto/crc32-pclmul_asm.S	2017-01-31 00:00:43.444637796 +0100
@@ -102,6 +103,12 @@
  *	                     size_t len, uint crc32)
  */
 
+#ifndef __x86_64__
+__i686_get_pc_thunk_cx:
+	mov     (%esp),%ecx
+	ret
+#endif
+
 ENTRY(crc32_pclmul_le_16) /* buffer and buffer size are 16 bytes aligned */
 	movdqa  (BUF), %xmm1
 	movdqa  0x10(BUF), %xmm2
@@ -113,9 +120,8 @@ ENTRY(crc32_pclmul_le_16) /* buffer and
 	add     $0x40, BUF
 #ifndef __x86_64__
 	/* This is for position independent code(-fPIC) support for 32bit */
-	call    delta
+	call    __i686_get_pc_thunk_cx
 delta:
-	pop     %ecx
 #endif
 	cmp     $0x40, LEN
 	jb      less_64
@@ -123,7 +129,7 @@ delta:
 #ifdef __x86_64__
 	movdqa .Lconstant_R2R1(%rip), CONSTANT
 #else
-	movdqa .Lconstant_R2R1 - delta(%ecx), CONSTANT
+	movdqa %cs:.Lconstant_R2R1 - delta (%ecx), CONSTANT
 #endif
 
 loop_64:/*  64 bytes Full cache line folding */
@@ -172,7 +178,7 @@ less_64:/*  Folding cache line into 128b
 #ifdef __x86_64__
 	movdqa  .Lconstant_R4R3(%rip), CONSTANT
 #else
-	movdqa  .Lconstant_R4R3 - delta(%ecx), CONSTANT
+	movdqa  %cs:.Lconstant_R4R3 - delta(%ecx), CONSTANT
 #endif
 	prefetchnta     (BUF)
 
@@ -220,8 +226,8 @@ fold_64:
 	movdqa  .Lconstant_R5(%rip), CONSTANT
 	movdqa  .Lconstant_mask32(%rip), %xmm3
 #else
-	movdqa  .Lconstant_R5 - delta(%ecx), CONSTANT
-	movdqa  .Lconstant_mask32 - delta(%ecx), %xmm3
+	movdqa  %cs:.Lconstant_R5 - delta(%ecx), CONSTANT
+	movdqa  %cs:.Lconstant_mask32 - delta(%ecx), %xmm3
 #endif
 	psrldq  $0x04, %xmm2
 	pand    %xmm3, %xmm1
@@ -232,7 +238,7 @@ fold_64:
 #ifdef __x86_64__
 	movdqa  .Lconstant_RUpoly(%rip), CONSTANT
 #else
-	movdqa  .Lconstant_RUpoly - delta(%ecx), CONSTANT
+	movdqa  %cs:.Lconstant_RUpoly - delta(%ecx), CONSTANT
 #endif
 	movdqa  %xmm1, %xmm2
 	pand    %xmm3, %xmm1
