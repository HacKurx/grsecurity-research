diff --git a/arch/x86/crypto/aes-i586-asm_32.S b/arch/x86/crypto/aes-i586-asm_32.S
index 2849dbc..d7ff39c 100644
--- a/arch/x86/crypto/aes-i586-asm_32.S
+++ b/arch/x86/crypto/aes-i586-asm_32.S
@@ -38,6 +38,7 @@
 
 #include <linux/linkage.h>
 #include <asm/asm-offsets.h>
+#include <asm/alternative-asm.h>
 
 #define tlen 1024   // length of each of 4 'xor' arrays (256 32-bit words)
 
@@ -286,7 +287,7 @@ ENTRY(aes_enc_blk)
 	pop     %ebx
 	mov     %r0,(%ebp)
 	pop     %ebp
-	ret
+	pax_ret aes_enc_blk
 ENDPROC(aes_enc_blk)
 
 // AES (Rijndael) Decryption Subroutine
@@ -358,5 +359,5 @@ ENTRY(aes_dec_blk)
 	pop     %ebx
 	mov     %r0,(%ebp)
 	pop     %ebp
-	ret
+	pax_ret aes_dec_blk
 ENDPROC(aes_dec_blk)
diff --git a/arch/x86/crypto/aes-x86_64-asm_64.S b/arch/x86/crypto/aes-x86_64-asm_64.S
index 9105655..cf81747 100644
--- a/arch/x86/crypto/aes-x86_64-asm_64.S
+++ b/arch/x86/crypto/aes-x86_64-asm_64.S
@@ -8,6 +8,8 @@
  * including this sentence is retained in full.
  */
 
+#include <asm/alternative-asm.h>
+
 .extern crypto_ft_tab
 .extern crypto_it_tab
 .extern crypto_fl_tab
@@ -77,7 +79,7 @@
 	movl	r6 ## E,4(r9);		\
 	movl	r7 ## E,8(r9);		\
 	movl	r8 ## E,12(r9);		\
-	ret;				\
+	pax_ret FUNC;			\
 	ENDPROC(FUNC);
 
 #define round(TAB,OFFSET,r1,r2,r3,r4,r5,r6,r7,r8,ra,rb,rc,rd) \
diff --git a/arch/x86/crypto/aes_ctrby8_avx-x86_64.S b/arch/x86/crypto/aes_ctrby8_avx-x86_64.S
index a916c4a..7e7b7cf 100644
--- a/arch/x86/crypto/aes_ctrby8_avx-x86_64.S
+++ b/arch/x86/crypto/aes_ctrby8_avx-x86_64.S
@@ -64,6 +64,7 @@
 
 #include <linux/linkage.h>
 #include <asm/inst.h>
+#include <asm/alternative-asm.h>
 
 #define CONCAT(a,b)	a##b
 #define VMOVDQ		vmovdqu
@@ -436,7 +437,7 @@ ddq_add_8:
 
 /* main body of aes ctr load */
 
-.macro do_aes_ctrmain key_len
+.macro do_aes_ctrmain func key_len
 	cmp	$16, num_bytes
 	jb	.Ldo_return2\key_len
 
@@ -537,7 +538,7 @@ ddq_add_8:
 	/* return updated IV */
 	vpshufb	xbyteswap, xcounter, xcounter
 	vmovdqu	xcounter, (p_iv)
-	ret
+	pax_ret \func
 .endm
 
 /*
@@ -549,7 +550,7 @@ ddq_add_8:
  */
 ENTRY(aes_ctr_enc_128_avx_by8)
 	/* call the aes main loop */
-	do_aes_ctrmain KEY_128
+	do_aes_ctrmain aes_ctr_enc_128_avx_by8 KEY_128
 
 ENDPROC(aes_ctr_enc_128_avx_by8)
 
@@ -562,7 +563,7 @@ ENDPROC(aes_ctr_enc_128_avx_by8)
  */
 ENTRY(aes_ctr_enc_192_avx_by8)
 	/* call the aes main loop */
-	do_aes_ctrmain KEY_192
+	do_aes_ctrmain aes_ctr_enc_192_avx_by8 KEY_192
 
 ENDPROC(aes_ctr_enc_192_avx_by8)
 
@@ -575,6 +576,6 @@ ENDPROC(aes_ctr_enc_192_avx_by8)
  */
 ENTRY(aes_ctr_enc_256_avx_by8)
 	/* call the aes main loop */
-	do_aes_ctrmain KEY_256
+	do_aes_ctrmain aes_ctr_enc_256_avx_by8 KEY_256
 
 ENDPROC(aes_ctr_enc_256_avx_by8)
diff --git a/arch/x86/crypto/aesni-intel_asm.S b/arch/x86/crypto/aesni-intel_asm.S
index 383a6f8..dc7f45d 100644
--- a/arch/x86/crypto/aesni-intel_asm.S
+++ b/arch/x86/crypto/aesni-intel_asm.S
@@ -32,6 +32,7 @@
 #include <linux/linkage.h>
 #include <asm/inst.h>
 #include <asm/frame.h>
+#include <asm/alternative-asm.h>
 
 /*
  * The following macros are used to move an (un)aligned 16 byte value to/from
@@ -218,7 +219,7 @@ enc:        .octa 0x2
 * num_initial_blocks = b mod 4
 * encrypt the initial num_initial_blocks blocks and apply ghash on
 * the ciphertext
-* %r10, %r11, %r12, %rax, %xmm5, %xmm6, %xmm7, %xmm8, %xmm9 registers
+* %r10, %r11, %r15, %rax, %xmm5, %xmm6, %xmm7, %xmm8, %xmm9 registers
 * are clobbered
 * arg1, %arg2, %arg3, %r14 are used as a pointer only, not modified
 */
@@ -228,8 +229,8 @@ enc:        .octa 0x2
 XMM2 XMM3 XMM4 XMMDst TMP6 TMP7 i i_seq operation
         MOVADQ     SHUF_MASK(%rip), %xmm14
 	mov	   arg7, %r10           # %r10 = AAD
-	mov	   arg8, %r12           # %r12 = aadLen
-	mov	   %r12, %r11
+	mov	   arg8, %r15           # %r15 = aadLen
+	mov	   %r15, %r11
 	pxor	   %xmm\i, %xmm\i
 
 _get_AAD_loop\num_initial_blocks\operation:
@@ -238,17 +239,17 @@ _get_AAD_loop\num_initial_blocks\operation:
 	psrldq	   $4, %xmm\i
 	pxor	   \TMP1, %xmm\i
 	add	   $4, %r10
-	sub	   $4, %r12
+	sub	   $4, %r15
 	jne	   _get_AAD_loop\num_initial_blocks\operation
 
 	cmp	   $16, %r11
 	je	   _get_AAD_loop2_done\num_initial_blocks\operation
 
-	mov	   $16, %r12
+	mov	   $16, %r15
 _get_AAD_loop2\num_initial_blocks\operation:
 	psrldq	   $4, %xmm\i
-	sub	   $4, %r12
-	cmp	   %r11, %r12
+	sub	   $4, %r15
+	cmp	   %r11, %r15
 	jne	   _get_AAD_loop2\num_initial_blocks\operation
 
 _get_AAD_loop2_done\num_initial_blocks\operation:
@@ -443,7 +444,7 @@ _initial_blocks_done\num_initial_blocks\operation:
 * num_initial_blocks = b mod 4
 * encrypt the initial num_initial_blocks blocks and apply ghash on
 * the ciphertext
-* %r10, %r11, %r12, %rax, %xmm5, %xmm6, %xmm7, %xmm8, %xmm9 registers
+* %r10, %r11, %r15, %rax, %xmm5, %xmm6, %xmm7, %xmm8, %xmm9 registers
 * are clobbered
 * arg1, %arg2, %arg3, %r14 are used as a pointer only, not modified
 */
@@ -453,8 +454,8 @@ _initial_blocks_done\num_initial_blocks\operation:
 XMM2 XMM3 XMM4 XMMDst TMP6 TMP7 i i_seq operation
         MOVADQ     SHUF_MASK(%rip), %xmm14
 	mov	   arg7, %r10           # %r10 = AAD
-	mov	   arg8, %r12           # %r12 = aadLen
-	mov	   %r12, %r11
+	mov	   arg8, %r15           # %r15 = aadLen
+	mov	   %r15, %r11
 	pxor	   %xmm\i, %xmm\i
 _get_AAD_loop\num_initial_blocks\operation:
 	movd	   (%r10), \TMP1
@@ -462,15 +463,15 @@ _get_AAD_loop\num_initial_blocks\operation:
 	psrldq	   $4, %xmm\i
 	pxor	   \TMP1, %xmm\i
 	add	   $4, %r10
-	sub	   $4, %r12
+	sub	   $4, %r15
 	jne	   _get_AAD_loop\num_initial_blocks\operation
 	cmp	   $16, %r11
 	je	   _get_AAD_loop2_done\num_initial_blocks\operation
-	mov	   $16, %r12
+	mov	   $16, %r15
 _get_AAD_loop2\num_initial_blocks\operation:
 	psrldq	   $4, %xmm\i
-	sub	   $4, %r12
-	cmp	   %r11, %r12
+	sub	   $4, %r15
+	cmp	   %r11, %r15
 	jne	   _get_AAD_loop2\num_initial_blocks\operation
 _get_AAD_loop2_done\num_initial_blocks\operation:
 	PSHUFB_XMM   %xmm14, %xmm\i # byte-reflect the AAD data
@@ -1280,8 +1281,8 @@ _esb_loop_\@:
 * poly = x^128 + x^127 + x^126 + x^121 + 1
 *
 *****************************************************************************/
-ENTRY(aesni_gcm_dec)
-	push	%r12
+RAP_ENTRY(aesni_gcm_dec)
+	push	%r15
 	push	%r13
 	push	%r14
 	mov	%rsp, %r14
@@ -1291,8 +1292,8 @@ ENTRY(aesni_gcm_dec)
 */
 	sub	$VARIABLE_OFFSET, %rsp
 	and	$~63, %rsp                        # align rsp to 64 bytes
-	mov	%arg6, %r12
-	movdqu	(%r12), %xmm13			  # %xmm13 = HashKey
+	mov	%arg6, %r15
+	movdqu	(%r15), %xmm13			  # %xmm13 = HashKey
         movdqa  SHUF_MASK(%rip), %xmm2
 	PSHUFB_XMM %xmm2, %xmm13
 
@@ -1320,10 +1321,10 @@ ENTRY(aesni_gcm_dec)
 	movdqa %xmm13, HashKey(%rsp)           # store HashKey<<1 (mod poly)
 	mov %arg4, %r13    # save the number of bytes of plaintext/ciphertext
 	and $-16, %r13                      # %r13 = %r13 - (%r13 mod 16)
-	mov %r13, %r12
-	and $(3<<4), %r12
+	mov %r13, %r15
+	and $(3<<4), %r15
 	jz _initial_num_blocks_is_0_decrypt
-	cmp $(2<<4), %r12
+	cmp $(2<<4), %r15
 	jb _initial_num_blocks_is_1_decrypt
 	je _initial_num_blocks_is_2_decrypt
 _initial_num_blocks_is_3_decrypt:
@@ -1373,16 +1374,16 @@ _zero_cipher_left_decrypt:
 	sub $16, %r11
 	add %r13, %r11
 	movdqu (%arg3,%r11,1), %xmm1   # receive the last <16 byte block
-	lea SHIFT_MASK+16(%rip), %r12
-	sub %r13, %r12
+	lea SHIFT_MASK+16(%rip), %r15
+	sub %r13, %r15
 # adjust the shuffle mask pointer to be able to shift 16-%r13 bytes
 # (%r13 is the number of bytes in plaintext mod 16)
-	movdqu (%r12), %xmm2           # get the appropriate shuffle mask
+	movdqu (%r15), %xmm2           # get the appropriate shuffle mask
 	PSHUFB_XMM %xmm2, %xmm1            # right shift 16-%r13 butes
 
 	movdqa  %xmm1, %xmm2
 	pxor %xmm1, %xmm0            # Ciphertext XOR E(K, Yn)
-	movdqu ALL_F-SHIFT_MASK(%r12), %xmm1
+	movdqu ALL_F-SHIFT_MASK(%r15), %xmm1
 	# get the appropriate mask to mask out top 16-%r13 bytes of %xmm0
 	pand %xmm1, %xmm0            # mask out top 16-%r13 bytes of %xmm0
 	pand    %xmm1, %xmm2
@@ -1411,9 +1412,9 @@ _less_than_8_bytes_left_decrypt:
 	sub	$1, %r13
 	jne	_less_than_8_bytes_left_decrypt
 _multiple_of_16_bytes_decrypt:
-	mov	arg8, %r12		  # %r13 = aadLen (number of bytes)
-	shl	$3, %r12		  # convert into number of bits
-	movd	%r12d, %xmm15		  # len(A) in %xmm15
+	mov	arg8, %r15		  # %r13 = aadLen (number of bytes)
+	shl	$3, %r15		  # convert into number of bits
+	movd	%r15d, %xmm15		  # len(A) in %xmm15
 	shl	$3, %arg4		  # len(C) in bits (*128)
 	MOVQ_R64_XMM	%arg4, %xmm1
 	pslldq	$8, %xmm15		  # %xmm15 = len(A)||0x0000000000000000
@@ -1452,8 +1453,8 @@ _return_T_done_decrypt:
 	mov	%r14, %rsp
 	pop	%r14
 	pop	%r13
-	pop	%r12
-	ret
+	pop	%r15
+	pax_ret aesni_gcm_dec
 ENDPROC(aesni_gcm_dec)
 
 
@@ -1540,8 +1541,8 @@ ENDPROC(aesni_gcm_dec)
 *
 * poly = x^128 + x^127 + x^126 + x^121 + 1
 ***************************************************************************/
-ENTRY(aesni_gcm_enc)
-	push	%r12
+RAP_ENTRY(aesni_gcm_enc)
+	push	%r15
 	push	%r13
 	push	%r14
 	mov	%rsp, %r14
@@ -1551,8 +1552,8 @@ ENTRY(aesni_gcm_enc)
 #
 	sub	$VARIABLE_OFFSET, %rsp
 	and	$~63, %rsp
-	mov	%arg6, %r12
-	movdqu	(%r12), %xmm13
+	mov	%arg6, %r15
+	movdqu	(%r15), %xmm13
         movdqa  SHUF_MASK(%rip), %xmm2
 	PSHUFB_XMM %xmm2, %xmm13
 
@@ -1576,13 +1577,13 @@ ENTRY(aesni_gcm_enc)
 	movdqa	%xmm13, HashKey(%rsp)
 	mov	%arg4, %r13            # %xmm13 holds HashKey<<1 (mod poly)
 	and	$-16, %r13
-	mov	%r13, %r12
+	mov	%r13, %r15
 
         # Encrypt first few blocks
 
-	and	$(3<<4), %r12
+	and	$(3<<4), %r15
 	jz	_initial_num_blocks_is_0_encrypt
-	cmp	$(2<<4), %r12
+	cmp	$(2<<4), %r15
 	jb	_initial_num_blocks_is_1_encrypt
 	je	_initial_num_blocks_is_2_encrypt
 _initial_num_blocks_is_3_encrypt:
@@ -1635,14 +1636,14 @@ _zero_cipher_left_encrypt:
 	sub $16, %r11
 	add %r13, %r11
 	movdqu (%arg3,%r11,1), %xmm1     # receive the last <16 byte blocks
-	lea SHIFT_MASK+16(%rip), %r12
-	sub %r13, %r12
+	lea SHIFT_MASK+16(%rip), %r15
+	sub %r13, %r15
 	# adjust the shuffle mask pointer to be able to shift 16-r13 bytes
 	# (%r13 is the number of bytes in plaintext mod 16)
-	movdqu	(%r12), %xmm2           # get the appropriate shuffle mask
+	movdqu	(%r15), %xmm2           # get the appropriate shuffle mask
 	PSHUFB_XMM	%xmm2, %xmm1            # shift right 16-r13 byte
 	pxor	%xmm1, %xmm0            # Plaintext XOR Encrypt(K, Yn)
-	movdqu	ALL_F-SHIFT_MASK(%r12), %xmm1
+	movdqu	ALL_F-SHIFT_MASK(%r15), %xmm1
 	# get the appropriate mask to mask out top 16-r13 bytes of xmm0
 	pand	%xmm1, %xmm0            # mask out top 16-r13 bytes of xmm0
         movdqa SHUF_MASK(%rip), %xmm10
@@ -1675,9 +1676,9 @@ _less_than_8_bytes_left_encrypt:
 	sub $1, %r13
 	jne _less_than_8_bytes_left_encrypt
 _multiple_of_16_bytes_encrypt:
-	mov	arg8, %r12    # %r12 = addLen (number of bytes)
-	shl	$3, %r12
-	movd	%r12d, %xmm15       # len(A) in %xmm15
+	mov	arg8, %r15    # %r15 = addLen (number of bytes)
+	shl	$3, %r15
+	movd	%r15d, %xmm15       # len(A) in %xmm15
 	shl	$3, %arg4               # len(C) in bits (*128)
 	MOVQ_R64_XMM	%arg4, %xmm1
 	pslldq	$8, %xmm15          # %xmm15 = len(A)||0x0000000000000000
@@ -1716,8 +1717,8 @@ _return_T_done_encrypt:
 	mov	%r14, %rsp
 	pop	%r14
 	pop	%r13
-	pop	%r12
-	ret
+	pop	%r15
+	pax_ret aesni_gcm_enc
 ENDPROC(aesni_gcm_enc)
 
 #endif
@@ -1734,7 +1735,7 @@ _key_expansion_256a:
 	pxor %xmm1, %xmm0
 	movaps %xmm0, (TKEYP)
 	add $0x10, TKEYP
-	ret
+	pax_ret _key_expansion_128
 ENDPROC(_key_expansion_128)
 ENDPROC(_key_expansion_256a)
 
@@ -1760,7 +1761,7 @@ _key_expansion_192a:
 	shufps $0b01001110, %xmm2, %xmm1
 	movaps %xmm1, 0x10(TKEYP)
 	add $0x20, TKEYP
-	ret
+	pax_ret _key_expansion_192a
 ENDPROC(_key_expansion_192a)
 
 .align 4
@@ -1780,7 +1781,7 @@ _key_expansion_192b:
 
 	movaps %xmm0, (TKEYP)
 	add $0x10, TKEYP
-	ret
+	pax_ret _key_expansion_192b
 ENDPROC(_key_expansion_192b)
 
 .align 4
@@ -1793,7 +1794,7 @@ _key_expansion_256b:
 	pxor %xmm1, %xmm2
 	movaps %xmm2, (TKEYP)
 	add $0x10, TKEYP
-	ret
+	pax_ret _key_expansion_256b
 ENDPROC(_key_expansion_256b)
 
 /*
@@ -1820,72 +1821,72 @@ ENTRY(aesni_set_key)
 	movaps %xmm2, (TKEYP)
 	add $0x10, TKEYP
 	AESKEYGENASSIST 0x1 %xmm2 %xmm1		# round 1
-	call _key_expansion_256a
+	pax_direct_call _key_expansion_256a
 	AESKEYGENASSIST 0x1 %xmm0 %xmm1
-	call _key_expansion_256b
+	pax_direct_call _key_expansion_256b
 	AESKEYGENASSIST 0x2 %xmm2 %xmm1		# round 2
-	call _key_expansion_256a
+	pax_direct_call _key_expansion_256a
 	AESKEYGENASSIST 0x2 %xmm0 %xmm1
-	call _key_expansion_256b
+	pax_direct_call _key_expansion_256b
 	AESKEYGENASSIST 0x4 %xmm2 %xmm1		# round 3
-	call _key_expansion_256a
+	pax_direct_call _key_expansion_256a
 	AESKEYGENASSIST 0x4 %xmm0 %xmm1
-	call _key_expansion_256b
+	pax_direct_call _key_expansion_256b
 	AESKEYGENASSIST 0x8 %xmm2 %xmm1		# round 4
-	call _key_expansion_256a
+	pax_direct_call _key_expansion_256a
 	AESKEYGENASSIST 0x8 %xmm0 %xmm1
-	call _key_expansion_256b
+	pax_direct_call _key_expansion_256b
 	AESKEYGENASSIST 0x10 %xmm2 %xmm1	# round 5
-	call _key_expansion_256a
+	pax_direct_call _key_expansion_256a
 	AESKEYGENASSIST 0x10 %xmm0 %xmm1
-	call _key_expansion_256b
+	pax_direct_call _key_expansion_256b
 	AESKEYGENASSIST 0x20 %xmm2 %xmm1	# round 6
-	call _key_expansion_256a
+	pax_direct_call _key_expansion_256a
 	AESKEYGENASSIST 0x20 %xmm0 %xmm1
-	call _key_expansion_256b
+	pax_direct_call _key_expansion_256b
 	AESKEYGENASSIST 0x40 %xmm2 %xmm1	# round 7
-	call _key_expansion_256a
+	pax_direct_call _key_expansion_256a
 	jmp .Ldec_key
 .Lenc_key192:
 	movq 0x10(UKEYP), %xmm2		# other user key
 	AESKEYGENASSIST 0x1 %xmm2 %xmm1		# round 1
-	call _key_expansion_192a
+	pax_direct_call _key_expansion_192a
 	AESKEYGENASSIST 0x2 %xmm2 %xmm1		# round 2
-	call _key_expansion_192b
+	pax_direct_call _key_expansion_192b
 	AESKEYGENASSIST 0x4 %xmm2 %xmm1		# round 3
-	call _key_expansion_192a
+	pax_direct_call _key_expansion_192a
 	AESKEYGENASSIST 0x8 %xmm2 %xmm1		# round 4
-	call _key_expansion_192b
+	pax_direct_call _key_expansion_192b
 	AESKEYGENASSIST 0x10 %xmm2 %xmm1	# round 5
-	call _key_expansion_192a
+	pax_direct_call _key_expansion_192a
 	AESKEYGENASSIST 0x20 %xmm2 %xmm1	# round 6
-	call _key_expansion_192b
+	pax_direct_call _key_expansion_192b
 	AESKEYGENASSIST 0x40 %xmm2 %xmm1	# round 7
-	call _key_expansion_192a
+	pax_direct_call _key_expansion_192a
 	AESKEYGENASSIST 0x80 %xmm2 %xmm1	# round 8
-	call _key_expansion_192b
+	pax_direct_call _key_expansion_192b
 	jmp .Ldec_key
 .Lenc_key128:
 	AESKEYGENASSIST 0x1 %xmm0 %xmm1		# round 1
-	call _key_expansion_128
+	pax_direct_call _key_expansion_128
 	AESKEYGENASSIST 0x2 %xmm0 %xmm1		# round 2
-	call _key_expansion_128
+	pax_direct_call _key_expansion_128
 	AESKEYGENASSIST 0x4 %xmm0 %xmm1		# round 3
-	call _key_expansion_128
+	pax_direct_call _key_expansion_128
 	AESKEYGENASSIST 0x8 %xmm0 %xmm1		# round 4
-	call _key_expansion_128
+	pax_direct_call _key_expansion_128
 	AESKEYGENASSIST 0x10 %xmm0 %xmm1	# round 5
-	call _key_expansion_128
+	pax_direct_call _key_expansion_128
 	AESKEYGENASSIST 0x20 %xmm0 %xmm1	# round 6
-	call _key_expansion_128
+	pax_direct_call _key_expansion_128
 	AESKEYGENASSIST 0x40 %xmm0 %xmm1	# round 7
-	call _key_expansion_128
+	pax_direct_call _key_expansion_128
 	AESKEYGENASSIST 0x80 %xmm0 %xmm1	# round 8
-	call _key_expansion_128
+	pax_direct_call _key_expansion_128
 	AESKEYGENASSIST 0x1b %xmm0 %xmm1	# round 9
-	call _key_expansion_128
+	pax_direct_call _key_expansion_128
 	AESKEYGENASSIST 0x36 %xmm0 %xmm1	# round 10
-	call _key_expansion_128
+	pax_direct_call _key_expansion_128
 .Ldec_key:
 	sub $0x10, TKEYP
 	movaps (KEYP), %xmm0
@@ -1908,13 +1909,13 @@ ENTRY(aesni_set_key)
 	popl KEYP
 #endif
 	FRAME_END
-	ret
+	pax_ret aesni_set_key
 ENDPROC(aesni_set_key)
 
 /*
  * void aesni_enc(struct crypto_aes_ctx *ctx, u8 *dst, const u8 *src)
  */
-ENTRY(aesni_enc)
+RAP_ENTRY(aesni_enc)
 	FRAME_BEGIN
 #ifndef __x86_64__
 	pushl KEYP
@@ -1925,14 +1926,14 @@ ENTRY(aesni_enc)
 #endif
 	movl 480(KEYP), KLEN		# key length
 	movups (INP), STATE		# input
-	call _aesni_enc1
+	pax_direct_call _aesni_enc1
 	movups STATE, (OUTP)		# output
 #ifndef __x86_64__
 	popl KLEN
 	popl KEYP
 #endif
 	FRAME_END
-	ret
+	pax_ret aesni_enc
 ENDPROC(aesni_enc)
 
 /*
@@ -1990,7 +1991,7 @@ _aesni_enc1:
 	AESENC KEY STATE
 	movaps 0x70(TKEYP), KEY
 	AESENCLAST KEY STATE
-	ret
+	pax_ret _aesni_enc1
 ENDPROC(_aesni_enc1)
 
 /*
@@ -2099,13 +2100,13 @@ _aesni_enc4:
 	AESENCLAST KEY STATE2
 	AESENCLAST KEY STATE3
 	AESENCLAST KEY STATE4
-	ret
+	pax_ret _aesni_enc4
 ENDPROC(_aesni_enc4)
 
 /*
  * void aesni_dec (struct crypto_aes_ctx *ctx, u8 *dst, const u8 *src)
  */
-ENTRY(aesni_dec)
+RAP_ENTRY(aesni_dec)
 	FRAME_BEGIN
 #ifndef __x86_64__
 	pushl KEYP
@@ -2117,14 +2118,14 @@ ENTRY(aesni_dec)
 	mov 480(KEYP), KLEN		# key length
 	add $240, KEYP
 	movups (INP), STATE		# input
-	call _aesni_dec1
+	pax_direct_call _aesni_dec1
 	movups STATE, (OUTP)		#output
 #ifndef __x86_64__
 	popl KLEN
 	popl KEYP
 #endif
 	FRAME_END
-	ret
+	pax_ret aesni_dec
 ENDPROC(aesni_dec)
 
 /*
@@ -2182,7 +2183,7 @@ _aesni_dec1:
 	AESDEC KEY STATE
 	movaps 0x70(TKEYP), KEY
 	AESDECLAST KEY STATE
-	ret
+	pax_ret _aesni_dec1
 ENDPROC(_aesni_dec1)
 
 /*
@@ -2291,7 +2292,7 @@ _aesni_dec4:
 	AESDECLAST KEY STATE2
 	AESDECLAST KEY STATE3
 	AESDECLAST KEY STATE4
-	ret
+	pax_ret _aesni_dec4
 ENDPROC(_aesni_dec4)
 
 /*
@@ -2322,7 +2323,7 @@ ENTRY(aesni_ecb_enc)
 	movups 0x10(INP), STATE2
 	movups 0x20(INP), STATE3
 	movups 0x30(INP), STATE4
-	call _aesni_enc4
+	pax_direct_call _aesni_enc4
 	movups STATE1, (OUTP)
 	movups STATE2, 0x10(OUTP)
 	movups STATE3, 0x20(OUTP)
@@ -2337,7 +2338,7 @@ ENTRY(aesni_ecb_enc)
 .align 4
 .Lecb_enc_loop1:
 	movups (INP), STATE1
-	call _aesni_enc1
+	pax_direct_call _aesni_enc1
 	movups STATE1, (OUTP)
 	sub $16, LEN
 	add $16, INP
@@ -2351,7 +2352,7 @@ ENTRY(aesni_ecb_enc)
 	popl LEN
 #endif
 	FRAME_END
-	ret
+	pax_ret aesni_ecb_enc
 ENDPROC(aesni_ecb_enc)
 
 /*
@@ -2383,7 +2384,7 @@ ENTRY(aesni_ecb_dec)
 	movups 0x10(INP), STATE2
 	movups 0x20(INP), STATE3
 	movups 0x30(INP), STATE4
-	call _aesni_dec4
+	pax_direct_call _aesni_dec4
 	movups STATE1, (OUTP)
 	movups STATE2, 0x10(OUTP)
 	movups STATE3, 0x20(OUTP)
@@ -2398,7 +2399,7 @@ ENTRY(aesni_ecb_dec)
 .align 4
 .Lecb_dec_loop1:
 	movups (INP), STATE1
-	call _aesni_dec1
+	pax_direct_call _aesni_dec1
 	movups STATE1, (OUTP)
 	sub $16, LEN
 	add $16, INP
@@ -2412,7 +2413,7 @@ ENTRY(aesni_ecb_dec)
 	popl LEN
 #endif
 	FRAME_END
-	ret
+	pax_ret aesni_ecb_dec
 ENDPROC(aesni_ecb_dec)
 
 /*
@@ -2440,7 +2441,7 @@ ENTRY(aesni_cbc_enc)
 .Lcbc_enc_loop:
 	movups (INP), IN	# load input
 	pxor IN, STATE
-	call _aesni_enc1
+	pax_direct_call _aesni_enc1
 	movups STATE, (OUTP)	# store output
 	sub $16, LEN
 	add $16, INP
@@ -2456,7 +2457,7 @@ ENTRY(aesni_cbc_enc)
 	popl IVP
 #endif
 	FRAME_END
-	ret
+	pax_ret aesni_cbc_enc
 ENDPROC(aesni_cbc_enc)
 
 /*
@@ -2500,7 +2501,7 @@ ENTRY(aesni_cbc_dec)
 	movups 0x30(INP), IN2
 	movaps IN2, STATE4
 #endif
-	call _aesni_dec4
+	pax_direct_call _aesni_dec4
 	pxor IV, STATE1
 #ifdef __x86_64__
 	pxor IN1, STATE2
@@ -2530,7 +2531,7 @@ ENTRY(aesni_cbc_dec)
 .Lcbc_dec_loop1:
 	movups (INP), IN
 	movaps IN, STATE
-	call _aesni_dec1
+	pax_direct_call _aesni_dec1
 	pxor IV, STATE
 	movups STATE, (OUTP)
 	movaps IN, IV
@@ -2549,7 +2550,7 @@ ENTRY(aesni_cbc_dec)
 	popl IVP
 #endif
 	FRAME_END
-	ret
+	pax_ret aesni_cbc_dec
 ENDPROC(aesni_cbc_dec)
 
 #ifdef __x86_64__
@@ -2578,7 +2579,7 @@ _aesni_inc_init:
 	mov $1, TCTR_LOW
 	MOVQ_R64_XMM TCTR_LOW INC
 	MOVQ_R64_XMM CTR TCTR_LOW
-	ret
+	pax_ret _aesni_inc_init
 ENDPROC(_aesni_inc_init)
 
 /*
@@ -2607,37 +2608,37 @@ _aesni_inc:
 .Linc_low:
 	movaps CTR, IV
 	PSHUFB_XMM BSWAP_MASK IV
-	ret
+	pax_ret _aesni_inc
 ENDPROC(_aesni_inc)
 
 /*
  * void aesni_ctr_enc(struct crypto_aes_ctx *ctx, const u8 *dst, u8 *src,
  *		      size_t len, u8 *iv)
  */
-ENTRY(aesni_ctr_enc)
+RAP_ENTRY(aesni_ctr_enc)
 	FRAME_BEGIN
 	cmp $16, LEN
 	jb .Lctr_enc_just_ret
 	mov 480(KEYP), KLEN
 	movups (IVP), IV
-	call _aesni_inc_init
+	pax_direct_call _aesni_inc_init
 	cmp $64, LEN
 	jb .Lctr_enc_loop1
 .align 4
 .Lctr_enc_loop4:
 	movaps IV, STATE1
-	call _aesni_inc
+	pax_direct_call _aesni_inc
 	movups (INP), IN1
 	movaps IV, STATE2
-	call _aesni_inc
+	pax_direct_call _aesni_inc
 	movups 0x10(INP), IN2
 	movaps IV, STATE3
-	call _aesni_inc
+	pax_direct_call _aesni_inc
 	movups 0x20(INP), IN3
 	movaps IV, STATE4
-	call _aesni_inc
+	pax_direct_call _aesni_inc
 	movups 0x30(INP), IN4
-	call _aesni_enc4
+	pax_direct_call _aesni_enc4
 	pxor IN1, STATE1
 	movups STATE1, (OUTP)
 	pxor IN2, STATE2
@@ -2656,9 +2657,9 @@ ENTRY(aesni_ctr_enc)
 .align 4
 .Lctr_enc_loop1:
 	movaps IV, STATE
-	call _aesni_inc
+	pax_direct_call _aesni_inc
 	movups (INP), IN
-	call _aesni_enc1
+	pax_direct_call _aesni_enc1
 	pxor IN, STATE
 	movups STATE, (OUTP)
 	sub $16, LEN
@@ -2670,7 +2671,7 @@ ENTRY(aesni_ctr_enc)
 	movups IV, (IVP)
 .Lctr_enc_just_ret:
 	FRAME_END
-	ret
+	pax_ret aesni_ctr_enc
 ENDPROC(aesni_ctr_enc)
 
 /*
@@ -2734,7 +2735,7 @@ ENTRY(aesni_xts_crypt8)
 	pxor INC, STATE4
 	movdqu IV, 0x30(OUTP)
 
-	call *%r11
+	pax_indirect_call "%r11", _aesni_enc4
 
 	movdqu 0x00(OUTP), INC
 	pxor INC, STATE1
@@ -2779,7 +2780,7 @@ ENTRY(aesni_xts_crypt8)
 	_aesni_gf128mul_x_ble()
 	movups IV, (IVP)
 
-	call *%r11
+	pax_indirect_call "%r11", _aesni_enc4
 
 	movdqu 0x40(OUTP), INC
 	pxor INC, STATE1
@@ -2798,7 +2799,7 @@ ENTRY(aesni_xts_crypt8)
 	movdqu STATE4, 0x70(OUTP)
 
 	FRAME_END
-	ret
+	pax_ret aesni_xts_crypt8
 ENDPROC(aesni_xts_crypt8)
 
 #endif
diff --git a/arch/x86/crypto/aesni-intel_avx-x86_64.S b/arch/x86/crypto/aesni-intel_avx-x86_64.S
index 522ab68..782ae42 100644
--- a/arch/x86/crypto/aesni-intel_avx-x86_64.S
+++ b/arch/x86/crypto/aesni-intel_avx-x86_64.S
@@ -121,6 +121,7 @@
 
 #include <linux/linkage.h>
 #include <asm/inst.h>
+#include <asm/alternative-asm.h>
 
 .data
 .align 16
@@ -1486,7 +1487,7 @@ ENTRY(aesni_gcm_precomp_avx_gen2)
         pop     %r14
         pop     %r13
         pop     %r12
-        ret
+        pax_ret aesni_gcm_precomp_avx_gen2
 ENDPROC(aesni_gcm_precomp_avx_gen2)
 
 ###############################################################################
@@ -1507,7 +1508,7 @@ ENDPROC(aesni_gcm_precomp_avx_gen2)
 ###############################################################################
 ENTRY(aesni_gcm_enc_avx_gen2)
         GCM_ENC_DEC_AVX     ENC
-	ret
+	pax_ret aesni_gcm_enc_avx_gen2
 ENDPROC(aesni_gcm_enc_avx_gen2)
 
 ###############################################################################
@@ -1528,7 +1529,7 @@ ENDPROC(aesni_gcm_enc_avx_gen2)
 ###############################################################################
 ENTRY(aesni_gcm_dec_avx_gen2)
         GCM_ENC_DEC_AVX     DEC
-	ret
+	pax_ret aesni_gcm_dec_avx_gen2
 ENDPROC(aesni_gcm_dec_avx_gen2)
 #endif /* CONFIG_AS_AVX */
 
@@ -2762,7 +2763,7 @@ ENTRY(aesni_gcm_precomp_avx_gen4)
         pop     %r14
         pop     %r13
         pop     %r12
-        ret
+        pax_ret aesni_gcm_precomp_avx_gen4
 ENDPROC(aesni_gcm_precomp_avx_gen4)
 
 
@@ -2784,7 +2785,7 @@ ENDPROC(aesni_gcm_precomp_avx_gen4)
 ###############################################################################
 ENTRY(aesni_gcm_enc_avx_gen4)
         GCM_ENC_DEC_AVX2     ENC
-	ret
+	pax_ret aesni_gcm_enc_avx_gen4
 ENDPROC(aesni_gcm_enc_avx_gen4)
 
 ###############################################################################
@@ -2805,7 +2806,7 @@ ENDPROC(aesni_gcm_enc_avx_gen4)
 ###############################################################################
 ENTRY(aesni_gcm_dec_avx_gen4)
         GCM_ENC_DEC_AVX2     DEC
-	ret
+	pax_ret aesni_gcm_dec_avx_gen4
 ENDPROC(aesni_gcm_dec_avx_gen4)
 
 #endif /* CONFIG_AS_AVX2 */
diff --git a/arch/x86/crypto/aesni-intel_glue.c b/arch/x86/crypto/aesni-intel_glue.c
index aa8b067..f9da224 100644
--- a/arch/x86/crypto/aesni-intel_glue.c
+++ b/arch/x86/crypto/aesni-intel_glue.c
@@ -83,6 +83,15 @@ asmlinkage void aesni_cbc_enc(struct crypto_aes_ctx *ctx, u8 *out,
 			      const u8 *in, unsigned int len, u8 *iv);
 asmlinkage void aesni_cbc_dec(struct crypto_aes_ctx *ctx, u8 *out,
 			      const u8 *in, unsigned int len, u8 *iv);
+int _key_expansion_128(struct crypto_aes_ctx *ctx, const u8 *in_key, unsigned int key_len) __rap_hash;
+int _key_expansion_192a(struct crypto_aes_ctx *ctx, const u8 *in_key, unsigned int key_len) __rap_hash;
+int _key_expansion_192b(struct crypto_aes_ctx *ctx, const u8 *in_key, unsigned int key_len) __rap_hash;
+int _key_expansion_256a(struct crypto_aes_ctx *ctx, const u8 *in_key, unsigned int key_len) __rap_hash;
+int _key_expansion_256b(struct crypto_aes_ctx *ctx, const u8 *in_key, unsigned int key_len) __rap_hash;
+void _aesni_enc1(void *ctx, u8 *out, const u8 *in) __rap_hash;
+void _aesni_enc4(void *ctx, u8 *out, const u8 *in) __rap_hash;
+void _aesni_dec1(void *ctx, u8 *out, const u8 *in) __rap_hash;
+void _aesni_dec4(void *ctx, u8 *out, const u8 *in) __rap_hash;
 
 int crypto_fpu_init(void);
 void crypto_fpu_exit(void);
@@ -96,6 +105,8 @@ static void (*aesni_ctr_enc_tfm)(struct crypto_aes_ctx *ctx, u8 *out,
 			      const u8 *in, unsigned int len, u8 *iv);
 asmlinkage void aesni_ctr_enc(struct crypto_aes_ctx *ctx, u8 *out,
 			      const u8 *in, unsigned int len, u8 *iv);
+void _aesni_inc(struct crypto_aes_ctx *ctx, u8 *out, const u8 *in, unsigned int len, u8 *iv) __rap_hash;
+void _aesni_inc_init(struct crypto_aes_ctx *ctx, u8 *out, const u8 *in, unsigned int len, u8 *iv) __rap_hash;
 
 asmlinkage void aesni_xts_crypt8(struct crypto_aes_ctx *ctx, u8 *out,
 				 const u8 *in, bool enc, u8 *iv);
diff --git a/arch/x86/crypto/blowfish-x86_64-asm_64.S b/arch/x86/crypto/blowfish-x86_64-asm_64.S
index 246c670..d4e1aa5 100644
--- a/arch/x86/crypto/blowfish-x86_64-asm_64.S
+++ b/arch/x86/crypto/blowfish-x86_64-asm_64.S
@@ -21,6 +21,7 @@
  */
 
 #include <linux/linkage.h>
+#include <asm/alternative-asm.h>
 
 .file "blowfish-x86_64-asm.S"
 .text
@@ -149,13 +150,13 @@ ENTRY(__blowfish_enc_blk)
 	jnz .L__enc_xor;
 
 	write_block();
-	ret;
+	pax_ret __blowfish_enc_blk;
 .L__enc_xor:
 	xor_block();
-	ret;
+	pax_ret __blowfish_enc_blk;
 ENDPROC(__blowfish_enc_blk)
 
-ENTRY(blowfish_dec_blk)
+RAP_ENTRY(blowfish_dec_blk)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst
@@ -183,7 +184,7 @@ ENTRY(blowfish_dec_blk)
 
 	movq %r11, %rbp;
 
-	ret;
+	pax_ret blowfish_dec_blk;
 ENDPROC(blowfish_dec_blk)
 
 /**********************************************************************
@@ -334,17 +335,17 @@ ENTRY(__blowfish_enc_blk_4way)
 
 	popq %rbx;
 	popq %rbp;
-	ret;
+	pax_ret __blowfish_enc_blk_4way;
 
 .L__enc_xor4:
 	xor_block4();
 
 	popq %rbx;
 	popq %rbp;
-	ret;
+	pax_ret __blowfish_enc_blk_4way;
 ENDPROC(__blowfish_enc_blk_4way)
 
-ENTRY(blowfish_dec_blk_4way)
+RAP_ENTRY(blowfish_dec_blk_4way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst
@@ -375,5 +376,5 @@ ENTRY(blowfish_dec_blk_4way)
 	popq %rbx;
 	popq %rbp;
 
-	ret;
+	pax_ret blowfish_dec_blk_4way;
 ENDPROC(blowfish_dec_blk_4way)
diff --git a/arch/x86/crypto/camellia-aesni-avx-asm_64.S b/arch/x86/crypto/camellia-aesni-avx-asm_64.S
index aa9e8bd..7e68f75 100644
--- a/arch/x86/crypto/camellia-aesni-avx-asm_64.S
+++ b/arch/x86/crypto/camellia-aesni-avx-asm_64.S
@@ -17,6 +17,7 @@
 
 #include <linux/linkage.h>
 #include <asm/frame.h>
+#include <asm/alternative-asm.h>
 
 #define CAMELLIA_TABLE_BYTE_LEN 272
 
@@ -192,7 +193,7 @@ roundsm16_x0_x1_x2_x3_x4_x5_x6_x7_y0_y1_y2_y3_y4_y5_y6_y7_cd:
 	roundsm16(%xmm0, %xmm1, %xmm2, %xmm3, %xmm4, %xmm5, %xmm6, %xmm7,
 		  %xmm8, %xmm9, %xmm10, %xmm11, %xmm12, %xmm13, %xmm14, %xmm15,
 		  %rcx, (%r9));
-	ret;
+	pax_ret roundsm16_x0_x1_x2_x3_x4_x5_x6_x7_y0_y1_y2_y3_y4_y5_y6_y7_cd;
 ENDPROC(roundsm16_x0_x1_x2_x3_x4_x5_x6_x7_y0_y1_y2_y3_y4_y5_y6_y7_cd)
 
 .align 8
@@ -200,7 +201,7 @@ roundsm16_x4_x5_x6_x7_x0_x1_x2_x3_y4_y5_y6_y7_y0_y1_y2_y3_ab:
 	roundsm16(%xmm4, %xmm5, %xmm6, %xmm7, %xmm0, %xmm1, %xmm2, %xmm3,
 		  %xmm12, %xmm13, %xmm14, %xmm15, %xmm8, %xmm9, %xmm10, %xmm11,
 		  %rax, (%r9));
-	ret;
+	pax_ret roundsm16_x4_x5_x6_x7_x0_x1_x2_x3_y4_y5_y6_y7_y0_y1_y2_y3_ab;
 ENDPROC(roundsm16_x4_x5_x6_x7_x0_x1_x2_x3_y4_y5_y6_y7_y0_y1_y2_y3_ab)
 
 /*
@@ -212,7 +213,7 @@ ENDPROC(roundsm16_x4_x5_x6_x7_x0_x1_x2_x3_y4_y5_y6_y7_y0_y1_y2_y3_ab)
 #define two_roundsm16(x0, x1, x2, x3, x4, x5, x6, x7, y0, y1, y2, y3, y4, y5, \
 		      y6, y7, mem_ab, mem_cd, i, dir, store_ab) \
 	leaq (key_table + (i) * 8)(CTX), %r9; \
-	call roundsm16_x0_x1_x2_x3_x4_x5_x6_x7_y0_y1_y2_y3_y4_y5_y6_y7_cd; \
+	pax_direct_call roundsm16_x0_x1_x2_x3_x4_x5_x6_x7_y0_y1_y2_y3_y4_y5_y6_y7_cd; \
 	\
 	vmovdqu x4, 0 * 16(mem_cd); \
 	vmovdqu x5, 1 * 16(mem_cd); \
@@ -224,7 +225,7 @@ ENDPROC(roundsm16_x4_x5_x6_x7_x0_x1_x2_x3_y4_y5_y6_y7_y0_y1_y2_y3_ab)
 	vmovdqu x3, 7 * 16(mem_cd); \
 	\
 	leaq (key_table + ((i) + (dir)) * 8)(CTX), %r9; \
-	call roundsm16_x4_x5_x6_x7_x0_x1_x2_x3_y4_y5_y6_y7_y0_y1_y2_y3_ab; \
+	pax_direct_call roundsm16_x4_x5_x6_x7_x0_x1_x2_x3_y4_y5_y6_y7_y0_y1_y2_y3_ab; \
 	\
 	store_ab(x0, x1, x2, x3, x4, x5, x6, x7, mem_ab);
 
@@ -783,7 +784,7 @@ __camellia_enc_blk16:
 		    %xmm15, (key_table)(CTX, %r8, 8), (%rax), 1 * 16(%rax));
 
 	FRAME_END
-	ret;
+	pax_ret camellia_xts_enc_16way;
 
 .align 8
 .Lenc_max32:
@@ -870,7 +871,7 @@ __camellia_dec_blk16:
 		    %xmm15, (key_table)(CTX), (%rax), 1 * 16(%rax));
 
 	FRAME_END
-	ret;
+	pax_ret camellia_xts_dec_16way;
 
 .align 8
 .Ldec_max32:
@@ -889,7 +890,7 @@ __camellia_dec_blk16:
 	jmp .Ldec_max24;
 ENDPROC(__camellia_dec_blk16)
 
-ENTRY(camellia_ecb_enc_16way)
+RAP_ENTRY(camellia_ecb_enc_16way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst (16 blocks)
@@ -904,17 +905,17 @@ ENTRY(camellia_ecb_enc_16way)
 	/* now dst can be used as temporary buffer (even in src == dst case) */
 	movq	%rsi, %rax;
 
-	call __camellia_enc_blk16;
+	pax_direct_call __camellia_enc_blk16;
 
 	write_output(%xmm7, %xmm6, %xmm5, %xmm4, %xmm3, %xmm2, %xmm1, %xmm0,
 		     %xmm15, %xmm14, %xmm13, %xmm12, %xmm11, %xmm10, %xmm9,
 		     %xmm8, %rsi);
 
 	FRAME_END
-	ret;
+	pax_ret camellia_ecb_enc_16way;
 ENDPROC(camellia_ecb_enc_16way)
 
-ENTRY(camellia_ecb_dec_16way)
+RAP_ENTRY(camellia_ecb_dec_16way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst (16 blocks)
@@ -934,17 +935,17 @@ ENTRY(camellia_ecb_dec_16way)
 	/* now dst can be used as temporary buffer (even in src == dst case) */
 	movq	%rsi, %rax;
 
-	call __camellia_dec_blk16;
+	pax_direct_call __camellia_dec_blk16;
 
 	write_output(%xmm7, %xmm6, %xmm5, %xmm4, %xmm3, %xmm2, %xmm1, %xmm0,
 		     %xmm15, %xmm14, %xmm13, %xmm12, %xmm11, %xmm10, %xmm9,
 		     %xmm8, %rsi);
 
 	FRAME_END
-	ret;
+	pax_ret camellia_ecb_dec_16way;
 ENDPROC(camellia_ecb_dec_16way)
 
-ENTRY(camellia_cbc_dec_16way)
+RAP_ENTRY(camellia_cbc_dec_16way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst (16 blocks)
@@ -968,7 +969,7 @@ ENTRY(camellia_cbc_dec_16way)
 	subq $(16 * 16), %rsp;
 	movq %rsp, %rax;
 
-	call __camellia_dec_blk16;
+	pax_direct_call __camellia_dec_blk16;
 
 	addq $(16 * 16), %rsp;
 
@@ -992,7 +993,7 @@ ENTRY(camellia_cbc_dec_16way)
 		     %xmm8, %rsi);
 
 	FRAME_END
-	ret;
+	pax_ret camellia_cbc_dec_16way;
 ENDPROC(camellia_cbc_dec_16way)
 
 #define inc_le128(x, minus_one, tmp) \
@@ -1001,7 +1002,7 @@ ENDPROC(camellia_cbc_dec_16way)
 	vpslldq $8, tmp, tmp; \
 	vpsubq tmp, x, x;
 
-ENTRY(camellia_ctr_16way)
+RAP_ENTRY(camellia_ctr_16way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst (16 blocks)
@@ -1080,7 +1081,7 @@ ENTRY(camellia_ctr_16way)
 	vpxor 14 * 16(%rax), %xmm15, %xmm14;
 	vpxor 15 * 16(%rax), %xmm15, %xmm15;
 
-	call __camellia_enc_blk16;
+	pax_direct_call __camellia_enc_blk16;
 
 	addq $(16 * 16), %rsp;
 
@@ -1105,7 +1106,7 @@ ENTRY(camellia_ctr_16way)
 		     %xmm8, %rsi);
 
 	FRAME_END
-	ret;
+	pax_ret camellia_ctr_16way;
 ENDPROC(camellia_ctr_16way)
 
 #define gf128mul_x_ble(iv, mask, tmp) \
@@ -1224,7 +1225,7 @@ camellia_xts_crypt_16way:
 	vpxor 14 * 16(%rax), %xmm15, %xmm14;
 	vpxor 15 * 16(%rax), %xmm15, %xmm15;
 
-	call *%r9;
+	pax_indirect_call "%r9", camellia_xts_enc_16way;
 
 	addq $(16 * 16), %rsp;
 
@@ -1249,10 +1250,10 @@ camellia_xts_crypt_16way:
 		     %xmm8, %rsi);
 
 	FRAME_END
-	ret;
+	pax_ret camellia_xts_crypt_16way;
 ENDPROC(camellia_xts_crypt_16way)
 
-ENTRY(camellia_xts_enc_16way)
+RAP_ENTRY(camellia_xts_enc_16way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst (16 blocks)
@@ -1266,7 +1267,7 @@ ENTRY(camellia_xts_enc_16way)
 	jmp camellia_xts_crypt_16way;
 ENDPROC(camellia_xts_enc_16way)
 
-ENTRY(camellia_xts_dec_16way)
+RAP_ENTRY(camellia_xts_dec_16way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst (16 blocks)
diff --git a/arch/x86/crypto/camellia-aesni-avx2-asm_64.S b/arch/x86/crypto/camellia-aesni-avx2-asm_64.S
index 16186c1..a751452 100644
--- a/arch/x86/crypto/camellia-aesni-avx2-asm_64.S
+++ b/arch/x86/crypto/camellia-aesni-avx2-asm_64.S
@@ -12,6 +12,7 @@
 
 #include <linux/linkage.h>
 #include <asm/frame.h>
+#include <asm/alternative-asm.h>
 
 #define CAMELLIA_TABLE_BYTE_LEN 272
 
@@ -231,7 +232,7 @@ roundsm32_x0_x1_x2_x3_x4_x5_x6_x7_y0_y1_y2_y3_y4_y5_y6_y7_cd:
 	roundsm32(%ymm0, %ymm1, %ymm2, %ymm3, %ymm4, %ymm5, %ymm6, %ymm7,
 		  %ymm8, %ymm9, %ymm10, %ymm11, %ymm12, %ymm13, %ymm14, %ymm15,
 		  %rcx, (%r9));
-	ret;
+	pax_ret roundsm32_x0_x1_x2_x3_x4_x5_x6_x7_y0_y1_y2_y3_y4_y5_y6_y7_cd;
 ENDPROC(roundsm32_x0_x1_x2_x3_x4_x5_x6_x7_y0_y1_y2_y3_y4_y5_y6_y7_cd)
 
 .align 8
@@ -239,7 +240,7 @@ roundsm32_x4_x5_x6_x7_x0_x1_x2_x3_y4_y5_y6_y7_y0_y1_y2_y3_ab:
 	roundsm32(%ymm4, %ymm5, %ymm6, %ymm7, %ymm0, %ymm1, %ymm2, %ymm3,
 		  %ymm12, %ymm13, %ymm14, %ymm15, %ymm8, %ymm9, %ymm10, %ymm11,
 		  %rax, (%r9));
-	ret;
+	pax_ret roundsm32_x4_x5_x6_x7_x0_x1_x2_x3_y4_y5_y6_y7_y0_y1_y2_y3_ab;
 ENDPROC(roundsm32_x4_x5_x6_x7_x0_x1_x2_x3_y4_y5_y6_y7_y0_y1_y2_y3_ab)
 
 /*
@@ -251,7 +252,7 @@ ENDPROC(roundsm32_x4_x5_x6_x7_x0_x1_x2_x3_y4_y5_y6_y7_y0_y1_y2_y3_ab)
 #define two_roundsm32(x0, x1, x2, x3, x4, x5, x6, x7, y0, y1, y2, y3, y4, y5, \
 		      y6, y7, mem_ab, mem_cd, i, dir, store_ab) \
 	leaq (key_table + (i) * 8)(CTX), %r9; \
-	call roundsm32_x0_x1_x2_x3_x4_x5_x6_x7_y0_y1_y2_y3_y4_y5_y6_y7_cd; \
+	pax_direct_call roundsm32_x0_x1_x2_x3_x4_x5_x6_x7_y0_y1_y2_y3_y4_y5_y6_y7_cd; \
 	\
 	vmovdqu x0, 4 * 32(mem_cd); \
 	vmovdqu x1, 5 * 32(mem_cd); \
@@ -263,7 +264,7 @@ ENDPROC(roundsm32_x4_x5_x6_x7_x0_x1_x2_x3_y4_y5_y6_y7_y0_y1_y2_y3_ab)
 	vmovdqu x7, 3 * 32(mem_cd); \
 	\
 	leaq (key_table + ((i) + (dir)) * 8)(CTX), %r9; \
-	call roundsm32_x4_x5_x6_x7_x0_x1_x2_x3_y4_y5_y6_y7_y0_y1_y2_y3_ab; \
+	pax_direct_call roundsm32_x4_x5_x6_x7_x0_x1_x2_x3_y4_y5_y6_y7_y0_y1_y2_y3_ab; \
 	\
 	store_ab(x0, x1, x2, x3, x4, x5, x6, x7, mem_ab);
 
@@ -823,7 +824,7 @@ __camellia_enc_blk32:
 		    %ymm15, (key_table)(CTX, %r8, 8), (%rax), 1 * 32(%rax));
 
 	FRAME_END
-	ret;
+	pax_ret __camellia_enc_blk32;
 
 .align 8
 .Lenc_max32:
@@ -910,7 +911,7 @@ __camellia_dec_blk32:
 		    %ymm15, (key_table)(CTX), (%rax), 1 * 32(%rax));
 
 	FRAME_END
-	ret;
+	pax_ret __camellia_dec_blk32;
 
 .align 8
 .Ldec_max32:
@@ -929,7 +930,7 @@ __camellia_dec_blk32:
 	jmp .Ldec_max24;
 ENDPROC(__camellia_dec_blk32)
 
-ENTRY(camellia_ecb_enc_32way)
+RAP_ENTRY(camellia_ecb_enc_32way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst (32 blocks)
@@ -946,7 +947,7 @@ ENTRY(camellia_ecb_enc_32way)
 	/* now dst can be used as temporary buffer (even in src == dst case) */
 	movq	%rsi, %rax;
 
-	call __camellia_enc_blk32;
+	pax_direct_call __camellia_enc_blk32;
 
 	write_output(%ymm7, %ymm6, %ymm5, %ymm4, %ymm3, %ymm2, %ymm1, %ymm0,
 		     %ymm15, %ymm14, %ymm13, %ymm12, %ymm11, %ymm10, %ymm9,
@@ -955,10 +956,10 @@ ENTRY(camellia_ecb_enc_32way)
 	vzeroupper;
 
 	FRAME_END
-	ret;
+	pax_ret camellia_ecb_enc_32way;
 ENDPROC(camellia_ecb_enc_32way)
 
-ENTRY(camellia_ecb_dec_32way)
+RAP_ENTRY(camellia_ecb_dec_32way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst (32 blocks)
@@ -980,7 +981,7 @@ ENTRY(camellia_ecb_dec_32way)
 	/* now dst can be used as temporary buffer (even in src == dst case) */
 	movq	%rsi, %rax;
 
-	call __camellia_dec_blk32;
+	pax_direct_call __camellia_dec_blk32;
 
 	write_output(%ymm7, %ymm6, %ymm5, %ymm4, %ymm3, %ymm2, %ymm1, %ymm0,
 		     %ymm15, %ymm14, %ymm13, %ymm12, %ymm11, %ymm10, %ymm9,
@@ -989,10 +990,10 @@ ENTRY(camellia_ecb_dec_32way)
 	vzeroupper;
 
 	FRAME_END
-	ret;
+	pax_ret camellia_ecb_dec_32way;
 ENDPROC(camellia_ecb_dec_32way)
 
-ENTRY(camellia_cbc_dec_32way)
+RAP_ENTRY(camellia_cbc_dec_32way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst (32 blocks)
@@ -1028,7 +1029,7 @@ ENTRY(camellia_cbc_dec_32way)
 	movq %rsp, %rax;
 
 .Lcbc_dec_continue:
-	call __camellia_dec_blk32;
+	pax_direct_call __camellia_dec_blk32;
 
 	vmovdqu %ymm7, (%rax);
 	vpxor %ymm7, %ymm7, %ymm7;
@@ -1057,7 +1058,7 @@ ENTRY(camellia_cbc_dec_32way)
 	vzeroupper;
 
 	FRAME_END
-	ret;
+	pax_ret camellia_cbc_dec_32way;
 ENDPROC(camellia_cbc_dec_32way)
 
 #define inc_le128(x, minus_one, tmp) \
@@ -1074,7 +1075,7 @@ ENDPROC(camellia_cbc_dec_32way)
 	vpslldq $8, tmp1, tmp1; \
 	vpsubq tmp1, x, x;
 
-ENTRY(camellia_ctr_32way)
+RAP_ENTRY(camellia_ctr_32way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst (32 blocks)
@@ -1170,7 +1171,7 @@ ENTRY(camellia_ctr_32way)
 	vpxor 14 * 32(%rax), %ymm15, %ymm14;
 	vpxor 15 * 32(%rax), %ymm15, %ymm15;
 
-	call __camellia_enc_blk32;
+	pax_direct_call __camellia_enc_blk32;
 
 	movq %r10, %rsp;
 
@@ -1197,7 +1198,7 @@ ENTRY(camellia_ctr_32way)
 	vzeroupper;
 
 	FRAME_END
-	ret;
+	pax_ret camellia_ctr_32way;
 ENDPROC(camellia_ctr_32way)
 
 #define gf128mul_x_ble(iv, mask, tmp) \
@@ -1337,7 +1338,7 @@ camellia_xts_crypt_32way:
 	vpxor 14 * 32(%rax), %ymm15, %ymm14;
 	vpxor 15 * 32(%rax), %ymm15, %ymm15;
 
-	call *%r9;
+	pax_indirect_call "%r9", __camellia_enc_blk32;
 
 	addq $(16 * 32), %rsp;
 
@@ -1364,10 +1365,10 @@ camellia_xts_crypt_32way:
 	vzeroupper;
 
 	FRAME_END
-	ret;
+	pax_ret camellia_xts_crypt_32way;
 ENDPROC(camellia_xts_crypt_32way)
 
-ENTRY(camellia_xts_enc_32way)
+RAP_ENTRY(camellia_xts_enc_32way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst (32 blocks)
@@ -1382,7 +1383,7 @@ ENTRY(camellia_xts_enc_32way)
 	jmp camellia_xts_crypt_32way;
 ENDPROC(camellia_xts_enc_32way)
 
-ENTRY(camellia_xts_dec_32way)
+RAP_ENTRY(camellia_xts_dec_32way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst (32 blocks)
diff --git a/arch/x86/crypto/camellia-x86_64-asm_64.S b/arch/x86/crypto/camellia-x86_64-asm_64.S
index 310319c..4fa639a 100644
--- a/arch/x86/crypto/camellia-x86_64-asm_64.S
+++ b/arch/x86/crypto/camellia-x86_64-asm_64.S
@@ -21,6 +21,7 @@
  */
 
 #include <linux/linkage.h>
+#include <asm/alternative-asm.h>
 
 .file "camellia-x86_64-asm_64.S"
 .text
@@ -228,16 +229,16 @@ ENTRY(__camellia_enc_blk)
 	enc_outunpack(mov, RT1);
 
 	movq RRBP, %rbp;
-	ret;
+	pax_ret __camellia_enc_blk;
 
 .L__enc_xor:
 	enc_outunpack(xor, RT1);
 
 	movq RRBP, %rbp;
-	ret;
+	pax_ret __camellia_enc_blk;
 ENDPROC(__camellia_enc_blk)
 
-ENTRY(camellia_dec_blk)
+RAP_ENTRY(camellia_dec_blk)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst
@@ -272,7 +273,7 @@ ENTRY(camellia_dec_blk)
 	dec_outunpack();
 
 	movq RRBP, %rbp;
-	ret;
+	pax_ret camellia_dec_blk;
 ENDPROC(camellia_dec_blk)
 
 /**********************************************************************
@@ -463,17 +464,17 @@ ENTRY(__camellia_enc_blk_2way)
 
 	movq RRBP, %rbp;
 	popq %rbx;
-	ret;
+	pax_ret __camellia_enc_blk_2way;
 
 .L__enc2_xor:
 	enc_outunpack2(xor, RT2);
 
 	movq RRBP, %rbp;
 	popq %rbx;
-	ret;
+	pax_ret __camellia_enc_blk_2way;
 ENDPROC(__camellia_enc_blk_2way)
 
-ENTRY(camellia_dec_blk_2way)
+RAP_ENTRY(camellia_dec_blk_2way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst
@@ -510,5 +511,5 @@ ENTRY(camellia_dec_blk_2way)
 
 	movq RRBP, %rbp;
 	movq RXOR, %rbx;
-	ret;
+	pax_ret camellia_dec_blk_2way;
 ENDPROC(camellia_dec_blk_2way)
 
diff --git a/arch/x86/crypto/cast5-avx-x86_64-asm_64.S b/arch/x86/crypto/cast5-avx-x86_64-asm_64.S
index 14fa196..80d99b6 100644
--- a/arch/x86/crypto/cast5-avx-x86_64-asm_64.S
+++ b/arch/x86/crypto/cast5-avx-x86_64-asm_64.S
@@ -25,6 +25,7 @@
 
 #include <linux/linkage.h>
 #include <asm/frame.h>
+#include <asm/alternative-asm.h>
 
 .file "cast5-avx-x86_64-asm_64.S"
 
@@ -282,7 +283,7 @@ __cast5_enc_blk16:
 	outunpack_blocks(RR3, RL3, RTMP, RX, RKM);
 	outunpack_blocks(RR4, RL4, RTMP, RX, RKM);
 
-	ret;
+	pax_ret __cast5_enc_blk16;
 ENDPROC(__cast5_enc_blk16)
 
 .align 16
@@ -353,14 +354,14 @@ __cast5_dec_blk16:
 	outunpack_blocks(RR3, RL3, RTMP, RX, RKM);
 	outunpack_blocks(RR4, RL4, RTMP, RX, RKM);
 
-	ret;
+	pax_ret __cast5_dec_blk16;
 
 .L__skip_dec:
 	vpsrldq $4, RKR, RKR;
 	jmp .L__dec_tail;
 ENDPROC(__cast5_dec_blk16)
 
-ENTRY(cast5_ecb_enc_16way)
+RAP_ENTRY(cast5_ecb_enc_16way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst
@@ -379,7 +380,7 @@ ENTRY(cast5_ecb_enc_16way)
 	vmovdqu (6*4*4)(%rdx), RL4;
 	vmovdqu (7*4*4)(%rdx), RR4;
 
-	call __cast5_enc_blk16;
+	pax_direct_call __cast5_enc_blk16;
 
 	vmovdqu RR1, (0*4*4)(%r11);
 	vmovdqu RL1, (1*4*4)(%r11);
@@ -391,10 +392,10 @@ ENTRY(cast5_ecb_enc_16way)
 	vmovdqu RL4, (7*4*4)(%r11);
 
 	FRAME_END
-	ret;
+	pax_ret cast5_ecb_enc_16way;
 ENDPROC(cast5_ecb_enc_16way)
 
-ENTRY(cast5_ecb_dec_16way)
+RAP_ENTRY(cast5_ecb_dec_16way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst
@@ -413,7 +414,7 @@ ENTRY(cast5_ecb_dec_16way)
 	vmovdqu (6*4*4)(%rdx), RL4;
 	vmovdqu (7*4*4)(%rdx), RR4;
 
-	call __cast5_dec_blk16;
+	pax_direct_call __cast5_dec_blk16;
 
 	vmovdqu RR1, (0*4*4)(%r11);
 	vmovdqu RL1, (1*4*4)(%r11);
@@ -425,7 +426,7 @@ ENTRY(cast5_ecb_dec_16way)
 	vmovdqu RL4, (7*4*4)(%r11);
 
 	FRAME_END
-	ret;
+	pax_ret cast5_ecb_dec_16way;
 ENDPROC(cast5_ecb_dec_16way)
 
 ENTRY(cast5_cbc_dec_16way)
@@ -436,10 +437,10 @@ ENTRY(cast5_cbc_dec_16way)
 	 */
 	FRAME_BEGIN
 
-	pushq %r12;
+	pushq %r14;
 
 	movq %rsi, %r11;
-	movq %rdx, %r12;
+	movq %rdx, %r14;
 
 	vmovdqu (0*16)(%rdx), RL1;
 	vmovdqu (1*16)(%rdx), RR1;
@@ -450,19 +451,19 @@ ENTRY(cast5_cbc_dec_16way)
 	vmovdqu (6*16)(%rdx), RL4;
 	vmovdqu (7*16)(%rdx), RR4;
 
-	call __cast5_dec_blk16;
+	pax_direct_call __cast5_dec_blk16;
 
 	/* xor with src */
-	vmovq (%r12), RX;
+	vmovq (%r14), RX;
 	vpshufd $0x4f, RX, RX;
 	vpxor RX, RR1, RR1;
-	vpxor 0*16+8(%r12), RL1, RL1;
-	vpxor 1*16+8(%r12), RR2, RR2;
-	vpxor 2*16+8(%r12), RL2, RL2;
-	vpxor 3*16+8(%r12), RR3, RR3;
-	vpxor 4*16+8(%r12), RL3, RL3;
-	vpxor 5*16+8(%r12), RR4, RR4;
-	vpxor 6*16+8(%r12), RL4, RL4;
+	vpxor 0*16+8(%r14), RL1, RL1;
+	vpxor 1*16+8(%r14), RR2, RR2;
+	vpxor 2*16+8(%r14), RL2, RL2;
+	vpxor 3*16+8(%r14), RR3, RR3;
+	vpxor 4*16+8(%r14), RL3, RL3;
+	vpxor 5*16+8(%r14), RR4, RR4;
+	vpxor 6*16+8(%r14), RL4, RL4;
 
 	vmovdqu RR1, (0*16)(%r11);
 	vmovdqu RL1, (1*16)(%r11);
@@ -473,10 +474,10 @@ ENTRY(cast5_cbc_dec_16way)
 	vmovdqu RR4, (6*16)(%r11);
 	vmovdqu RL4, (7*16)(%r11);
 
-	popq %r12;
+	popq %r14;
 
 	FRAME_END
-	ret;
+	pax_ret cast5_cbc_dec_16way;
 ENDPROC(cast5_cbc_dec_16way)
 
 ENTRY(cast5_ctr_16way)
@@ -488,10 +489,10 @@ ENTRY(cast5_ctr_16way)
 	 */
 	FRAME_BEGIN
 
-	pushq %r12;
+	pushq %r14;
 
 	movq %rsi, %r11;
-	movq %rdx, %r12;
+	movq %rdx, %r14;
 
 	vpcmpeqd RTMP, RTMP, RTMP;
 	vpsrldq $8, RTMP, RTMP; /* low: -1, high: 0 */
@@ -528,17 +529,17 @@ ENTRY(cast5_ctr_16way)
 	vpshufb R1ST, RX, RX; /* be: IV16, IV16 */
 	vmovq RX, (%rcx);
 
-	call __cast5_enc_blk16;
+	pax_direct_call __cast5_enc_blk16;
 
 	/* dst = src ^ iv */
-	vpxor (0*16)(%r12), RR1, RR1;
-	vpxor (1*16)(%r12), RL1, RL1;
-	vpxor (2*16)(%r12), RR2, RR2;
-	vpxor (3*16)(%r12), RL2, RL2;
-	vpxor (4*16)(%r12), RR3, RR3;
-	vpxor (5*16)(%r12), RL3, RL3;
-	vpxor (6*16)(%r12), RR4, RR4;
-	vpxor (7*16)(%r12), RL4, RL4;
+	vpxor (0*16)(%r14), RR1, RR1;
+	vpxor (1*16)(%r14), RL1, RL1;
+	vpxor (2*16)(%r14), RR2, RR2;
+	vpxor (3*16)(%r14), RL2, RL2;
+	vpxor (4*16)(%r14), RR3, RR3;
+	vpxor (5*16)(%r14), RL3, RL3;
+	vpxor (6*16)(%r14), RR4, RR4;
+	vpxor (7*16)(%r14), RL4, RL4;
 	vmovdqu RR1, (0*16)(%r11);
 	vmovdqu RL1, (1*16)(%r11);
 	vmovdqu RR2, (2*16)(%r11);
@@ -548,8 +549,8 @@ ENTRY(cast5_ctr_16way)
 	vmovdqu RR4, (6*16)(%r11);
 	vmovdqu RL4, (7*16)(%r11);
 
-	popq %r12;
+	popq %r14;
 
 	FRAME_END
-	ret;
+	pax_ret cast5_ctr_16way;
 ENDPROC(cast5_ctr_16way)
diff --git a/arch/x86/crypto/cast5_avx_glue.c b/arch/x86/crypto/cast5_avx_glue.c
index 8648158..b56922a 100644
--- a/arch/x86/crypto/cast5_avx_glue.c
+++ b/arch/x86/crypto/cast5_avx_glue.c
@@ -44,6 +44,8 @@ asmlinkage void cast5_cbc_dec_16way(struct cast5_ctx *ctx, u8 *dst,
 				    const u8 *src);
 asmlinkage void cast5_ctr_16way(struct cast5_ctx *ctx, u8 *dst, const u8 *src,
 				__be64 *iv);
+void __cast5_enc_blk16(struct cast5_ctx *ctx, u8 *dst, const u8 *src) __rap_hash;
+void __cast5_dec_blk16(struct cast5_ctx *ctx, u8 *dst, const u8 *src) __rap_hash;
 
 static inline bool cast5_fpu_begin(bool fpu_enabled, unsigned int nbytes)
 {
diff --git a/arch/x86/crypto/cast6-avx-x86_64-asm_64.S b/arch/x86/crypto/cast6-avx-x86_64-asm_64.S
index c419389..7e2ed7c 100644
--- a/arch/x86/crypto/cast6-avx-x86_64-asm_64.S
+++ b/arch/x86/crypto/cast6-avx-x86_64-asm_64.S
@@ -25,6 +25,7 @@
 
 #include <linux/linkage.h>
 #include <asm/frame.h>
+#include <asm/alternative-asm.h>
 #include "glue_helper-asm-avx.S"
 
 .file "cast6-avx-x86_64-asm_64.S"
@@ -296,7 +297,7 @@ __cast6_enc_blk8:
 	outunpack_blocks(RA1, RB1, RC1, RD1, RTMP, RX, RKRF, RKM);
 	outunpack_blocks(RA2, RB2, RC2, RD2, RTMP, RX, RKRF, RKM);
 
-	ret;
+	pax_ret __cast6_enc_blk8;
 ENDPROC(__cast6_enc_blk8)
 
 .align 8
@@ -341,10 +342,10 @@ __cast6_dec_blk8:
 	outunpack_blocks(RA1, RB1, RC1, RD1, RTMP, RX, RKRF, RKM);
 	outunpack_blocks(RA2, RB2, RC2, RD2, RTMP, RX, RKRF, RKM);
 
-	ret;
+	pax_ret __cast6_dec_blk8;
 ENDPROC(__cast6_dec_blk8)
 
-ENTRY(cast6_ecb_enc_8way)
+RAP_ENTRY(cast6_ecb_enc_8way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst
@@ -356,15 +357,15 @@ ENTRY(cast6_ecb_enc_8way)
 
 	load_8way(%rdx, RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2);
 
-	call __cast6_enc_blk8;
+	pax_direct_call __cast6_enc_blk8;
 
 	store_8way(%r11, RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2);
 
 	FRAME_END
-	ret;
+	pax_ret cast6_ecb_enc_8way;
 ENDPROC(cast6_ecb_enc_8way)
 
-ENTRY(cast6_ecb_dec_8way)
+RAP_ENTRY(cast6_ecb_dec_8way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst
@@ -376,15 +377,15 @@ ENTRY(cast6_ecb_dec_8way)
 
 	load_8way(%rdx, RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2);
 
-	call __cast6_dec_blk8;
+	pax_direct_call __cast6_dec_blk8;
 
 	store_8way(%r11, RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2);
 
 	FRAME_END
-	ret;
+	pax_ret cast6_ecb_dec_8way;
 ENDPROC(cast6_ecb_dec_8way)
 
-ENTRY(cast6_cbc_dec_8way)
+RAP_ENTRY(cast6_cbc_dec_8way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst
@@ -392,24 +393,24 @@ ENTRY(cast6_cbc_dec_8way)
 	 */
 	FRAME_BEGIN
 
-	pushq %r12;
+	pushq %r14;
 
 	movq %rsi, %r11;
-	movq %rdx, %r12;
+	movq %rdx, %r14;
 
 	load_8way(%rdx, RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2);
 
-	call __cast6_dec_blk8;
+	pax_direct_call __cast6_dec_blk8;
 
-	store_cbc_8way(%r12, %r11, RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2);
+	store_cbc_8way(%r14, %r11, RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2);
 
-	popq %r12;
+	popq %r14;
 
 	FRAME_END
-	ret;
+	pax_ret cast6_cbc_dec_8way;
 ENDPROC(cast6_cbc_dec_8way)
 
-ENTRY(cast6_ctr_8way)
+RAP_ENTRY(cast6_ctr_8way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst
@@ -418,25 +419,25 @@ ENTRY(cast6_ctr_8way)
 	 */
 	FRAME_BEGIN
 
-	pushq %r12;
+	pushq %r14;
 
 	movq %rsi, %r11;
-	movq %rdx, %r12;
+	movq %rdx, %r14;
 
 	load_ctr_8way(%rcx, .Lbswap128_mask, RA1, RB1, RC1, RD1, RA2, RB2, RC2,
 		      RD2, RX, RKR, RKM);
 
-	call __cast6_enc_blk8;
+	pax_direct_call __cast6_enc_blk8;
 
-	store_ctr_8way(%r12, %r11, RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2);
+	store_ctr_8way(%r14, %r11, RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2);
 
-	popq %r12;
+	popq %r14;
 
 	FRAME_END
-	ret;
+	pax_ret cast6_ctr_8way;
 ENDPROC(cast6_ctr_8way)
 
-ENTRY(cast6_xts_enc_8way)
+RAP_ENTRY(cast6_xts_enc_8way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst
@@ -451,16 +452,16 @@ ENTRY(cast6_xts_enc_8way)
 	load_xts_8way(%rcx, %rdx, %rsi, RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2,
 		      RX, RKR, RKM, .Lxts_gf128mul_and_shl1_mask);
 
-	call __cast6_enc_blk8;
+	pax_direct_call __cast6_enc_blk8;
 
 	/* dst <= regs xor IVs(in dst) */
 	store_xts_8way(%r11, RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2);
 
 	FRAME_END
-	ret;
+	pax_ret cast6_xts_enc_8way;
 ENDPROC(cast6_xts_enc_8way)
 
-ENTRY(cast6_xts_dec_8way)
+RAP_ENTRY(cast6_xts_dec_8way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst
@@ -475,11 +476,11 @@ ENTRY(cast6_xts_dec_8way)
 	load_xts_8way(%rcx, %rdx, %rsi, RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2,
 		      RX, RKR, RKM, .Lxts_gf128mul_and_shl1_mask);
 
-	call __cast6_dec_blk8;
+	pax_direct_call __cast6_dec_blk8;
 
 	/* dst <= regs xor IVs(in dst) */
 	store_xts_8way(%r11, RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2);
 
 	FRAME_END
-	ret;
+	pax_ret cast6_xts_dec_8way;
 ENDPROC(cast6_xts_dec_8way)
diff --git a/arch/x86/crypto/chacha20-avx2-x86_64.S b/arch/x86/crypto/chacha20-avx2-x86_64.S
index 16694e6..4675b5e 100644
--- a/arch/x86/crypto/chacha20-avx2-x86_64.S
+++ b/arch/x86/crypto/chacha20-avx2-x86_64.S
@@ -10,6 +10,7 @@
  */
 
 #include <linux/linkage.h>
+#include <asm/alternative-asm.h>
 
 .data
 .align 32
@@ -439,5 +440,5 @@ ENTRY(chacha20_8block_xor_avx2)
 
 	vzeroupper
 	mov		%r8,%rsp
-	ret
+	pax_ret chacha20_8block_xor_avx2
 ENDPROC(chacha20_8block_xor_avx2)
diff --git a/arch/x86/crypto/chacha20-ssse3-x86_64.S b/arch/x86/crypto/chacha20-ssse3-x86_64.S
index 3a33124..ba21c6f 100644
--- a/arch/x86/crypto/chacha20-ssse3-x86_64.S
+++ b/arch/x86/crypto/chacha20-ssse3-x86_64.S
@@ -10,6 +10,7 @@
  */
 
 #include <linux/linkage.h>
+#include <asm/alternative-asm.h>
 
 .data
 .align 16
@@ -139,7 +140,7 @@ ENTRY(chacha20_block_xor_ssse3)
 	pxor		%xmm7,%xmm3
 	movdqu		%xmm3,0x30(%rsi)
 
-	ret
+	pax_ret chacha20_block_xor_ssse3
 ENDPROC(chacha20_block_xor_ssse3)
 
 ENTRY(chacha20_4block_xor_ssse3)
@@ -623,5 +624,5 @@ ENTRY(chacha20_4block_xor_ssse3)
 	movdqu		%xmm15,0xf0(%rsi)
 
 	mov		%r11,%rsp
-	ret
+	pax_ret chacha20_4block_xor_ssse3
 ENDPROC(chacha20_4block_xor_ssse3)
diff --git a/arch/x86/crypto/crc32-pclmul_asm.S b/arch/x86/crypto/crc32-pclmul_asm.S
index f247304..d253bd1 100644
--- a/arch/x86/crypto/crc32-pclmul_asm.S
+++ b/arch/x86/crypto/crc32-pclmul_asm.S
@@ -39,6 +39,7 @@
 
 #include <linux/linkage.h>
 #include <asm/inst.h>
+#include <asm/alternative-asm.h>
 
 
 .align 16
@@ -242,5 +248,5 @@ fold_64:
 	pxor    %xmm2, %xmm1
 	PEXTRD  0x01, %xmm1, %eax
 
-	ret
+	pax_ret crc32_pclmul_le_16
 ENDPROC(crc32_pclmul_le_16)
diff --git a/arch/x86/crypto/crc32c-pcl-intel-asm_64.S b/arch/x86/crypto/crc32c-pcl-intel-asm_64.S
index dc05f010..83302a8 100644
--- a/arch/x86/crypto/crc32c-pcl-intel-asm_64.S
+++ b/arch/x86/crypto/crc32c-pcl-intel-asm_64.S
@@ -45,6 +45,7 @@
 
 #include <asm/inst.h>
 #include <linux/linkage.h>
+#include <asm/alternative-asm.h>
 
 ## ISCSI CRC 32 Implementation with crc32 and pclmulqdq Instruction
 
@@ -309,7 +310,7 @@ do_return:
 	popq    %rsi
 	popq    %rdi
 	popq    %rbx
-        ret
+	pax_ret crc_pcl
 ENDPROC(crc_pcl)
 
 .section	.rodata, "a", %progbits
diff --git a/arch/x86/crypto/crct10dif-pcl-asm_64.S b/arch/x86/crypto/crct10dif-pcl-asm_64.S
index 35e9756..5048353 100644
--- a/arch/x86/crypto/crct10dif-pcl-asm_64.S
+++ b/arch/x86/crypto/crct10dif-pcl-asm_64.S
@@ -59,6 +59,7 @@
 #
 
 #include <linux/linkage.h>
+#include <asm/alternative-asm.h>
 
 .text
 
@@ -367,7 +368,7 @@ _cleanup:
 	# scale the result back to 16 bits
 	shr	$16, %eax
 	mov     %rcx, %rsp
-	ret
+	pax_ret crc_t10dif_pcl
 
 ########################################################################
 
diff --git a/arch/x86/crypto/des3_ede-asm_64.S b/arch/x86/crypto/des3_ede-asm_64.S
index 038f6ae..ec7142bf 100644
--- a/arch/x86/crypto/des3_ede-asm_64.S
+++ b/arch/x86/crypto/des3_ede-asm_64.S
@@ -15,6 +15,7 @@
  */
 
 #include <linux/linkage.h>
+#include <asm/alternative-asm.h>
 
 .file "des3_ede-asm_64.S"
 .text
@@ -250,7 +251,7 @@ ENTRY(des3_ede_x86_64_crypt_blk)
 	popq %rbx;
 	popq %rbp;
 
-	ret;
+	pax_ret des3_ede_x86_64_crypt_blk;
 ENDPROC(des3_ede_x86_64_crypt_blk)
 
 /***********************************************************************
@@ -534,7 +535,7 @@ ENTRY(des3_ede_x86_64_crypt_blk_3way)
 	popq %rbx;
 	popq %rbp;
 
-	ret;
+	pax_ret des3_ede_x86_64_crypt_blk_3way;
 ENDPROC(des3_ede_x86_64_crypt_blk_3way)
 
 .data
diff --git a/arch/x86/crypto/ghash-clmulni-intel_asm.S b/arch/x86/crypto/ghash-clmulni-intel_asm.S
index eed55c8..18f64dc 100644
--- a/arch/x86/crypto/ghash-clmulni-intel_asm.S
+++ b/arch/x86/crypto/ghash-clmulni-intel_asm.S
@@ -19,6 +19,7 @@
 #include <linux/linkage.h>
 #include <asm/inst.h>
 #include <asm/frame.h>
+#include <asm/alternative-asm.h>
 
 .data
 
@@ -90,7 +91,7 @@ __clmul_gf128mul_ble:
 	psrlq $1, T2
 	pxor T2, T1
 	pxor T1, DATA
-	ret
+	pax_ret __clmul_gf128mul_ble
 ENDPROC(__clmul_gf128mul_ble)
 
 /* void clmul_ghash_mul(char *dst, const u128 *shash) */
@@ -100,11 +101,11 @@ ENTRY(clmul_ghash_mul)
 	movups (%rsi), SHASH
 	movaps .Lbswap_mask, BSWAP
 	PSHUFB_XMM BSWAP DATA
-	call __clmul_gf128mul_ble
+	pax_direct_call __clmul_gf128mul_ble
 	PSHUFB_XMM BSWAP DATA
 	movups DATA, (%rdi)
 	FRAME_END
-	ret
+	pax_ret clmul_ghash_mul
 ENDPROC(clmul_ghash_mul)
 
 /*
@@ -124,7 +125,7 @@ ENTRY(clmul_ghash_update)
 	movups (%rsi), IN1
 	PSHUFB_XMM BSWAP IN1
 	pxor IN1, DATA
-	call __clmul_gf128mul_ble
+	pax_direct_call __clmul_gf128mul_ble
 	sub $16, %rdx
 	add $16, %rsi
 	cmp $16, %rdx
@@ -133,5 +134,5 @@ ENTRY(clmul_ghash_update)
 	movups DATA, (%rdi)
 .Lupdate_just_ret:
 	FRAME_END
-	ret
+	pax_ret clmul_ghash_update
 ENDPROC(clmul_ghash_update)
diff --git a/arch/x86/crypto/ghash-clmulni-intel_glue.c b/arch/x86/crypto/ghash-clmulni-intel_glue.c
index 0420bab..590ca78 100644
--- a/arch/x86/crypto/ghash-clmulni-intel_glue.c
+++ b/arch/x86/crypto/ghash-clmulni-intel_glue.c
@@ -26,6 +26,7 @@
 #define GHASH_DIGEST_SIZE	16
 
 void clmul_ghash_mul(char *dst, const u128 *shash);
+void __clmul_gf128mul_ble(char *dst, const u128 *shash) __rap_hash;
 
 void clmul_ghash_update(char *dst, const char *src, unsigned int srclen,
 			const u128 *shash);
diff --git a/arch/x86/crypto/poly1305-avx2-x86_64.S b/arch/x86/crypto/poly1305-avx2-x86_64.S
index eff2f41..932718e 100644
--- a/arch/x86/crypto/poly1305-avx2-x86_64.S
+++ b/arch/x86/crypto/poly1305-avx2-x86_64.S
@@ -10,6 +10,7 @@
  */
 
 #include <linux/linkage.h>
+#include <asm/alternative-asm.h>
 
 .data
 .align 32
@@ -382,5 +383,5 @@ ENTRY(poly1305_4block_avx2)
 	pop		%r13
 	pop		%r12
 	pop		%rbx
-	ret
+	pax_ret poly1305_4block_avx2
 ENDPROC(poly1305_4block_avx2)
diff --git a/arch/x86/crypto/poly1305-sse2-x86_64.S b/arch/x86/crypto/poly1305-sse2-x86_64.S
index 338c748..497359c 100644
--- a/arch/x86/crypto/poly1305-sse2-x86_64.S
+++ b/arch/x86/crypto/poly1305-sse2-x86_64.S
@@ -10,6 +10,7 @@
  */
 
 #include <linux/linkage.h>
+#include <asm/alternative-asm.h>
 
 .data
 .align 16
@@ -273,7 +274,7 @@ ENTRY(poly1305_block_sse2)
 	add		$0x10,%rsp
 	pop		%r12
 	pop		%rbx
-	ret
+	pax_ret poly1305_block_sse2
 ENDPROC(poly1305_block_sse2)
 
 
@@ -578,5 +579,5 @@ ENTRY(poly1305_2block_sse2)
 	pop		%r13
 	pop		%r12
 	pop		%rbx
-	ret
+	pax_ret poly1305_2block_sse2
 ENDPROC(poly1305_2block_sse2)
diff --git a/arch/x86/crypto/salsa20-i586-asm_32.S b/arch/x86/crypto/salsa20-i586-asm_32.S
index 329452b8..f136500 100644
--- a/arch/x86/crypto/salsa20-i586-asm_32.S
+++ b/arch/x86/crypto/salsa20-i586-asm_32.S
@@ -3,6 +3,7 @@
 # Public domain.
 
 #include <linux/linkage.h>
+#include <asm/alternative-asm.h>
 
 .text
 
@@ -924,7 +925,7 @@ ENTRY(salsa20_encrypt_bytes)
 	movl	96(%esp),%ebp
 	#     leave
 	add	%eax,%esp
-	ret
+	pax_ret salsa20_encrypt_bytes
 ._bytesatleast65:
 	#   bytes -= 64
 	sub	$64,%ebx
@@ -1059,7 +1060,7 @@ ENTRY(salsa20_keysetup)
 	movl	80(%esp),%ebp
 	# leave
 	add	%eax,%esp
-	ret
+	pax_ret salsa20_keysetup
 ENDPROC(salsa20_keysetup)
 
 # enter salsa20_ivsetup
@@ -1110,5 +1111,5 @@ ENTRY(salsa20_ivsetup)
 	movl	80(%esp),%ebp
 	# leave
 	add	%eax,%esp
-	ret
+	pax_ret salsa20_ivsetup
 ENDPROC(salsa20_ivsetup)
diff --git a/arch/x86/crypto/salsa20-x86_64-asm_64.S b/arch/x86/crypto/salsa20-x86_64-asm_64.S
index 9279e0b..6745d48 100644
--- a/arch/x86/crypto/salsa20-x86_64-asm_64.S
+++ b/arch/x86/crypto/salsa20-x86_64-asm_64.S
@@ -1,4 +1,5 @@
 #include <linux/linkage.h>
+#include <asm/alternative-asm.h>
 
 # enter salsa20_encrypt_bytes
 ENTRY(salsa20_encrypt_bytes)
@@ -789,7 +790,7 @@ ENTRY(salsa20_encrypt_bytes)
 	add	%r11,%rsp
 	mov	%rdi,%rax
 	mov	%rsi,%rdx
-	ret
+	pax_ret salsa20_encrypt_bytes
 #   bytesatleast65:
 ._bytesatleast65:
 	#   bytes -= 64
@@ -889,7 +890,7 @@ ENTRY(salsa20_keysetup)
 	add	%r11,%rsp
 	mov	%rdi,%rax
 	mov	%rsi,%rdx
-	ret
+	pax_ret salsa20_keysetup
 ENDPROC(salsa20_keysetup)
 
 # enter salsa20_ivsetup
@@ -914,5 +915,5 @@ ENTRY(salsa20_ivsetup)
 	add	%r11,%rsp
 	mov	%rdi,%rax
 	mov	%rsi,%rdx
-	ret
+	pax_ret salsa20_ivsetup
 ENDPROC(salsa20_ivsetup)
diff --git a/arch/x86/crypto/serpent-avx-x86_64-asm_64.S b/arch/x86/crypto/serpent-avx-x86_64-asm_64.S
index 8be5718..c5a9956 100644
--- a/arch/x86/crypto/serpent-avx-x86_64-asm_64.S
+++ b/arch/x86/crypto/serpent-avx-x86_64-asm_64.S
@@ -25,6 +25,7 @@
 
 #include <linux/linkage.h>
 #include <asm/frame.h>
+#include <asm/alternative-asm.h>
 #include "glue_helper-asm-avx.S"
 
 .file "serpent-avx-x86_64-asm_64.S"
@@ -619,7 +620,7 @@ __serpent_enc_blk8_avx:
 	write_blocks(RA1, RB1, RC1, RD1, RK0, RK1, RK2);
 	write_blocks(RA2, RB2, RC2, RD2, RK0, RK1, RK2);
 
-	ret;
+	pax_ret __serpent_enc_blk8_avx;
 ENDPROC(__serpent_enc_blk8_avx)
 
 .align 8
@@ -673,10 +674,10 @@ __serpent_dec_blk8_avx:
 	write_blocks(RC1, RD1, RB1, RE1, RK0, RK1, RK2);
 	write_blocks(RC2, RD2, RB2, RE2, RK0, RK1, RK2);
 
-	ret;
+	pax_ret __serpent_dec_blk8_avx;
 ENDPROC(__serpent_dec_blk8_avx)
 
-ENTRY(serpent_ecb_enc_8way_avx)
+RAP_ENTRY(serpent_ecb_enc_8way_avx)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst
@@ -686,15 +687,15 @@ ENTRY(serpent_ecb_enc_8way_avx)
 
 	load_8way(%rdx, RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2);
 
-	call __serpent_enc_blk8_avx;
+	pax_direct_call __serpent_enc_blk8_avx;
 
 	store_8way(%rsi, RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2);
 
 	FRAME_END
-	ret;
+	pax_ret serpent_ecb_enc_8way_avx;
 ENDPROC(serpent_ecb_enc_8way_avx)
 
-ENTRY(serpent_ecb_dec_8way_avx)
+RAP_ENTRY(serpent_ecb_dec_8way_avx)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst
@@ -704,15 +705,15 @@ ENTRY(serpent_ecb_dec_8way_avx)
 
 	load_8way(%rdx, RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2);
 
-	call __serpent_dec_blk8_avx;
+	pax_direct_call __serpent_dec_blk8_avx;
 
 	store_8way(%rsi, RC1, RD1, RB1, RE1, RC2, RD2, RB2, RE2);
 
 	FRAME_END
-	ret;
+	pax_ret serpent_ecb_dec_8way_avx;
 ENDPROC(serpent_ecb_dec_8way_avx)
 
-ENTRY(serpent_cbc_dec_8way_avx)
+RAP_ENTRY(serpent_cbc_dec_8way_avx)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst
@@ -722,15 +723,15 @@ ENTRY(serpent_cbc_dec_8way_avx)
 
 	load_8way(%rdx, RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2);
 
-	call __serpent_dec_blk8_avx;
+	pax_direct_call __serpent_dec_blk8_avx;
 
 	store_cbc_8way(%rdx, %rsi, RC1, RD1, RB1, RE1, RC2, RD2, RB2, RE2);
 
 	FRAME_END
-	ret;
+	pax_ret serpent_cbc_dec_8way_avx;
 ENDPROC(serpent_cbc_dec_8way_avx)
 
-ENTRY(serpent_ctr_8way_avx)
+RAP_ENTRY(serpent_ctr_8way_avx)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst
@@ -742,15 +743,15 @@ ENTRY(serpent_ctr_8way_avx)
 	load_ctr_8way(%rcx, .Lbswap128_mask, RA1, RB1, RC1, RD1, RA2, RB2, RC2,
 		      RD2, RK0, RK1, RK2);
 
-	call __serpent_enc_blk8_avx;
+	pax_direct_call __serpent_enc_blk8_avx;
 
 	store_ctr_8way(%rdx, %rsi, RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2);
 
 	FRAME_END
-	ret;
+	pax_ret serpent_ctr_8way_avx;
 ENDPROC(serpent_ctr_8way_avx)
 
-ENTRY(serpent_xts_enc_8way_avx)
+RAP_ENTRY(serpent_xts_enc_8way_avx)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst
@@ -763,16 +764,16 @@ ENTRY(serpent_xts_enc_8way_avx)
 	load_xts_8way(%rcx, %rdx, %rsi, RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2,
 		      RK0, RK1, RK2, .Lxts_gf128mul_and_shl1_mask);
 
-	call __serpent_enc_blk8_avx;
+	pax_direct_call __serpent_enc_blk8_avx;
 
 	/* dst <= regs xor IVs(in dst) */
 	store_xts_8way(%rsi, RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2);
 
 	FRAME_END
-	ret;
+	pax_ret serpent_xts_enc_8way_avx;
 ENDPROC(serpent_xts_enc_8way_avx)
 
-ENTRY(serpent_xts_dec_8way_avx)
+RAP_ENTRY(serpent_xts_dec_8way_avx)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst
@@ -785,11 +786,11 @@ ENTRY(serpent_xts_dec_8way_avx)
 	load_xts_8way(%rcx, %rdx, %rsi, RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2,
 		      RK0, RK1, RK2, .Lxts_gf128mul_and_shl1_mask);
 
-	call __serpent_dec_blk8_avx;
+	pax_direct_call __serpent_dec_blk8_avx;
 
 	/* dst <= regs xor IVs(in dst) */
 	store_xts_8way(%rsi, RC1, RD1, RB1, RE1, RC2, RD2, RB2, RE2);
 
 	FRAME_END
-	ret;
+	pax_ret serpent_xts_dec_8way_avx;
 ENDPROC(serpent_xts_dec_8way_avx)
diff --git a/arch/x86/crypto/serpent-avx2-asm_64.S b/arch/x86/crypto/serpent-avx2-asm_64.S
index 97c48ad..541b03c 100644
--- a/arch/x86/crypto/serpent-avx2-asm_64.S
+++ b/arch/x86/crypto/serpent-avx2-asm_64.S
@@ -16,6 +16,7 @@
 
 #include <linux/linkage.h>
 #include <asm/frame.h>
+#include <asm/alternative-asm.h>
 #include "glue_helper-asm-avx2.S"
 
 .file "serpent-avx2-asm_64.S"
@@ -611,7 +612,7 @@ __serpent_enc_blk16:
 	write_blocks(RA1, RB1, RC1, RD1, RK0, RK1, RK2);
 	write_blocks(RA2, RB2, RC2, RD2, RK0, RK1, RK2);
 
-	ret;
+	pax_ret __serpent_enc_blk16;
 ENDPROC(__serpent_enc_blk16)
 
 .align 8
@@ -665,10 +666,10 @@ __serpent_dec_blk16:
 	write_blocks(RC1, RD1, RB1, RE1, RK0, RK1, RK2);
 	write_blocks(RC2, RD2, RB2, RE2, RK0, RK1, RK2);
 
-	ret;
+	pax_ret __serpent_dec_blk16;
 ENDPROC(__serpent_dec_blk16)
 
-ENTRY(serpent_ecb_enc_16way)
+RAP_ENTRY(serpent_ecb_enc_16way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst
@@ -680,17 +681,17 @@ ENTRY(serpent_ecb_enc_16way)
 
 	load_16way(%rdx, RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2);
 
-	call __serpent_enc_blk16;
+	pax_direct_call __serpent_enc_blk16;
 
 	store_16way(%rsi, RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2);
 
 	vzeroupper;
 
 	FRAME_END
-	ret;
+	pax_ret serpent_ecb_enc_16way;
 ENDPROC(serpent_ecb_enc_16way)
 
-ENTRY(serpent_ecb_dec_16way)
+RAP_ENTRY(serpent_ecb_dec_16way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst
@@ -702,17 +703,17 @@ ENTRY(serpent_ecb_dec_16way)
 
 	load_16way(%rdx, RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2);
 
-	call __serpent_dec_blk16;
+	pax_direct_call __serpent_dec_blk16;
 
 	store_16way(%rsi, RC1, RD1, RB1, RE1, RC2, RD2, RB2, RE2);
 
 	vzeroupper;
 
 	FRAME_END
-	ret;
+	pax_ret serpent_ecb_dec_16way;
 ENDPROC(serpent_ecb_dec_16way)
 
-ENTRY(serpent_cbc_dec_16way)
+RAP_ENTRY(serpent_cbc_dec_16way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst
@@ -724,7 +725,7 @@ ENTRY(serpent_cbc_dec_16way)
 
 	load_16way(%rdx, RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2);
 
-	call __serpent_dec_blk16;
+	pax_direct_call __serpent_dec_blk16;
 
 	store_cbc_16way(%rdx, %rsi, RC1, RD1, RB1, RE1, RC2, RD2, RB2, RE2,
 			RK0);
@@ -732,10 +733,10 @@ ENTRY(serpent_cbc_dec_16way)
 	vzeroupper;
 
 	FRAME_END
-	ret;
+	pax_ret serpent_cbc_dec_16way;
 ENDPROC(serpent_cbc_dec_16way)
 
-ENTRY(serpent_ctr_16way)
+RAP_ENTRY(serpent_ctr_16way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst (16 blocks)
@@ -750,17 +751,17 @@ ENTRY(serpent_ctr_16way)
 		       RD2, RK0, RK0x, RK1, RK1x, RK2, RK2x, RK3, RK3x, RNOT,
 		       tp);
 
-	call __serpent_enc_blk16;
+	pax_direct_call __serpent_enc_blk16;
 
 	store_ctr_16way(%rdx, %rsi, RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2);
 
 	vzeroupper;
 
 	FRAME_END
-	ret;
+	pax_ret serpent_ctr_16way;
 ENDPROC(serpent_ctr_16way)
 
-ENTRY(serpent_xts_enc_16way)
+RAP_ENTRY(serpent_xts_enc_16way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst (16 blocks)
@@ -776,17 +777,17 @@ ENTRY(serpent_xts_enc_16way)
 		       .Lxts_gf128mul_and_shl1_mask_0,
 		       .Lxts_gf128mul_and_shl1_mask_1);
 
-	call __serpent_enc_blk16;
+	pax_direct_call __serpent_enc_blk16;
 
 	store_xts_16way(%rsi, RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2);
 
 	vzeroupper;
 
 	FRAME_END
-	ret;
+	pax_ret serpent_xts_enc_16way;
 ENDPROC(serpent_xts_enc_16way)
 
-ENTRY(serpent_xts_dec_16way)
+RAP_ENTRY(serpent_xts_dec_16way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst (16 blocks)
@@ -802,12 +803,12 @@ ENTRY(serpent_xts_dec_16way)
 		       .Lxts_gf128mul_and_shl1_mask_0,
 		       .Lxts_gf128mul_and_shl1_mask_1);
 
-	call __serpent_dec_blk16;
+	pax_direct_call __serpent_dec_blk16;
 
 	store_xts_16way(%rsi, RC1, RD1, RB1, RE1, RC2, RD2, RB2, RE2);
 
 	vzeroupper;
 
 	FRAME_END
-	ret;
+	pax_ret serpent_xts_dec_16way;
 ENDPROC(serpent_xts_dec_16way)
diff --git a/arch/x86/crypto/serpent-sse2-i586-asm_32.S b/arch/x86/crypto/serpent-sse2-i586-asm_32.S
index d348f15..48aa0c3 100644
--- a/arch/x86/crypto/serpent-sse2-i586-asm_32.S
+++ b/arch/x86/crypto/serpent-sse2-i586-asm_32.S
@@ -25,6 +25,7 @@
  */
 
 #include <linux/linkage.h>
+#include <asm/alternative-asm.h>
 
 .file "serpent-sse2-i586-asm_32.S"
 .text
@@ -568,12 +569,12 @@ ENTRY(__serpent_enc_blk_4way)
 
 	write_blocks(%eax, RA, RB, RC, RD, RT0, RT1, RE);
 
-	ret;
+	pax_ret __serpent_enc_blk_4way;
 
 .L__enc_xor4:
 	xor_blocks(%eax, RA, RB, RC, RD, RT0, RT1, RE);
 
-	ret;
+	pax_ret __serpent_enc_blk_4way;
 ENDPROC(__serpent_enc_blk_4way)
 
 ENTRY(serpent_dec_blk_4way)
@@ -627,5 +628,5 @@ ENTRY(serpent_dec_blk_4way)
 	movl arg_dst(%esp), %eax;
 	write_blocks(%eax, RC, RD, RB, RE, RT0, RT1, RA);
 
-	ret;
+	pax_ret serpent_dec_blk_4way;
 ENDPROC(serpent_dec_blk_4way)
diff --git a/arch/x86/crypto/serpent-sse2-x86_64-asm_64.S b/arch/x86/crypto/serpent-sse2-x86_64-asm_64.S
index acc066c..d96c7c2 100644
--- a/arch/x86/crypto/serpent-sse2-x86_64-asm_64.S
+++ b/arch/x86/crypto/serpent-sse2-x86_64-asm_64.S
@@ -25,6 +25,7 @@
  */
 
 #include <linux/linkage.h>
+#include <asm/alternative-asm.h>
 
 .file "serpent-sse2-x86_64-asm_64.S"
 .text
@@ -690,13 +691,13 @@ ENTRY(__serpent_enc_blk_8way)
 	write_blocks(%rsi, RA1, RB1, RC1, RD1, RK0, RK1, RK2);
 	write_blocks(%rax, RA2, RB2, RC2, RD2, RK0, RK1, RK2);
 
-	ret;
+	pax_ret __serpent_enc_blk_8way;
 
 .L__enc_xor8:
 	xor_blocks(%rsi, RA1, RB1, RC1, RD1, RK0, RK1, RK2);
 	xor_blocks(%rax, RA2, RB2, RC2, RD2, RK0, RK1, RK2);
 
-	ret;
+	pax_ret __serpent_enc_blk_8way;
 ENDPROC(__serpent_enc_blk_8way)
 
 ENTRY(serpent_dec_blk_8way)
@@ -750,5 +751,5 @@ ENTRY(serpent_dec_blk_8way)
 	write_blocks(%rsi, RC1, RD1, RB1, RE1, RK0, RK1, RK2);
 	write_blocks(%rax, RC2, RD2, RB2, RE2, RK0, RK1, RK2);
 
-	ret;
+	pax_ret serpent_dec_blk_8way;
 ENDPROC(serpent_dec_blk_8way)
diff --git a/arch/x86/crypto/serpent_sse2_glue.c b/arch/x86/crypto/serpent_sse2_glue.c
index 644f97a..4d069a1 100644
--- a/arch/x86/crypto/serpent_sse2_glue.c
+++ b/arch/x86/crypto/serpent_sse2_glue.c
@@ -45,8 +45,10 @@
 #include <asm/crypto/serpent-sse2.h>
 #include <asm/crypto/glue_helper.h>
 
-static void serpent_decrypt_cbc_xway(void *ctx, u128 *dst, const u128 *src)
+static void serpent_decrypt_cbc_xway(void *ctx, u8 *_dst, const u8 *_src)
 {
+	u128 *dst = (u128 *)_dst;
+	const u128 *src = (const u128 *)_src;
 	u128 ivs[SERPENT_PARALLEL_BLOCKS - 1];
 	unsigned int j;
 
diff --git a/arch/x86/crypto/sha1-mb/sha1_mb_mgr.h b/arch/x86/crypto/sha1-mb/sha1_mb_mgr.h
index 08ad1a9..293bc9e 100644
--- a/arch/x86/crypto/sha1-mb/sha1_mb_mgr.h
+++ b/arch/x86/crypto/sha1-mb/sha1_mb_mgr.h
@@ -106,5 +106,6 @@ struct job_sha1 *sha1_mb_mgr_submit_avx2(struct sha1_mb_mgr *state,
 					 struct job_sha1 *job);
 struct job_sha1 *sha1_mb_mgr_flush_avx2(struct sha1_mb_mgr *state);
 struct job_sha1 *sha1_mb_mgr_get_comp_job_avx2(struct sha1_mb_mgr *state);
+struct job_sha1 *sha1_x8_avx2(struct sha1_mb_mgr *state) __rap_hash;
 
 #endif
diff --git a/arch/x86/crypto/sha1-mb/sha1_mb_mgr_flush_avx2.S b/arch/x86/crypto/sha1-mb/sha1_mb_mgr_flush_avx2.S
index 96df6a3..f5f561f 100644
--- a/arch/x86/crypto/sha1-mb/sha1_mb_mgr_flush_avx2.S
+++ b/arch/x86/crypto/sha1-mb/sha1_mb_mgr_flush_avx2.S
@@ -53,6 +53,7 @@
  */
 #include <linux/linkage.h>
 #include <asm/frame.h>
+#include <asm/alternative-asm.h>
 #include "sha1_mb_mgr_datastruct.S"
 
 
@@ -103,7 +104,7 @@ offset = \_offset
 
 # JOB* sha1_mb_mgr_flush_avx2(MB_MGR *state)
 # arg 1 : rcx : state
-ENTRY(sha1_mb_mgr_flush_avx2)
+RAP_ENTRY(sha1_mb_mgr_flush_avx2)
 	FRAME_BEGIN
 	push	%rbx
 
@@ -183,7 +184,7 @@ LABEL skip_ %I
 
 	# "state" and "args" are the same address, arg1
 	# len is arg2
-	call	sha1_x8_avx2
+	pax_direct_call sha1_x8_avx2
 	# state and idx are intact
 
 
@@ -215,7 +216,7 @@ len_is_0:
 return:
 	pop	%rbx
 	FRAME_END
-	ret
+	pax_ret sha1_mb_mgr_flush_avx2
 
 return_null:
 	xor     job_rax, job_rax
@@ -226,7 +227,7 @@ ENDPROC(sha1_mb_mgr_flush_avx2)
 #################################################################
 
 .align 16
-ENTRY(sha1_mb_mgr_get_comp_job_avx2)
+RAP_ENTRY(sha1_mb_mgr_get_comp_job_avx2)
 	push    %rbx
 
 	## if bit 32+3 is set, then all lanes are empty
@@ -273,12 +274,12 @@ ENTRY(sha1_mb_mgr_get_comp_job_avx2)
 
 	pop     %rbx
 
-	ret
+	pax_ret sha1_mb_mgr_get_comp_job_avx2
 
 .return_null:
 	xor     job_rax, job_rax
 	pop     %rbx
-	ret
+	pax_ret sha1_mb_mgr_get_comp_job_avx2
 ENDPROC(sha1_mb_mgr_get_comp_job_avx2)
 
 .data
diff --git a/arch/x86/crypto/sha1-mb/sha1_mb_mgr_submit_avx2.S b/arch/x86/crypto/sha1-mb/sha1_mb_mgr_submit_avx2.S
index 63a0d9c..53b60ac 100644
--- a/arch/x86/crypto/sha1-mb/sha1_mb_mgr_submit_avx2.S
+++ b/arch/x86/crypto/sha1-mb/sha1_mb_mgr_submit_avx2.S
@@ -54,6 +54,7 @@
 
 #include <linux/linkage.h>
 #include <asm/frame.h>
+#include <asm/alternative-asm.h>
 #include "sha1_mb_mgr_datastruct.S"
 
 
@@ -98,7 +99,7 @@ lane_data       = %r10
 # JOB* submit_mb_mgr_submit_avx2(MB_MGR *state, job_sha1 *job)
 # arg 1 : rcx : state
 # arg 2 : rdx : job
-ENTRY(sha1_mb_mgr_submit_avx2)
+RAP_ENTRY(sha1_mb_mgr_submit_avx2)
 	FRAME_BEGIN
 	push	%rbx
 	push	%r12
@@ -163,7 +164,7 @@ start_loop:
 
 	# "state" and "args" are the same address, arg1
 	# len is arg2
-	call    sha1_x8_avx2
+	pax_direct_call sha1_x8_avx2
 
 	# state and idx are intact
 
@@ -195,7 +196,7 @@ return:
 	pop	%r12
 	pop	%rbx
 	FRAME_END
-	ret
+	pax_ret sha1_mb_mgr_submit_avx2
 
 return_null:
 	xor     job_rax, job_rax
diff --git a/arch/x86/crypto/sha1-mb/sha1_x8_avx2.S b/arch/x86/crypto/sha1-mb/sha1_x8_avx2.S
index c9dae1c..6055141 100644
--- a/arch/x86/crypto/sha1-mb/sha1_x8_avx2.S
+++ b/arch/x86/crypto/sha1-mb/sha1_x8_avx2.S
@@ -53,6 +53,7 @@
  */
 
 #include <linux/linkage.h>
+#include <asm/alternative-asm.h>
 #include "sha1_mb_mgr_datastruct.S"
 
 ## code to compute oct SHA1 using SSE-256
@@ -457,7 +458,7 @@ lloop:
 	pop	%r13
 	pop	%r12
 
-	ret
+	pax_ret sha1_x8_avx2
 ENDPROC(sha1_x8_avx2)
 
 
diff --git a/arch/x86/crypto/sha1_avx2_x86_64_asm.S b/arch/x86/crypto/sha1_avx2_x86_64_asm.S
index 1cd792d..2236003 100644
--- a/arch/x86/crypto/sha1_avx2_x86_64_asm.S
+++ b/arch/x86/crypto/sha1_avx2_x86_64_asm.S
@@ -70,6 +70,7 @@
  */
 
 #include <linux/linkage.h>
+#include <asm/alternative-asm.h>
 
 #define	CTX	%rdi	/* arg1 */
 #define BUF	%rsi	/* arg2 */
@@ -671,7 +672,7 @@ _loop3:
 	pop	%rbp
 	pop	%rbx
 
-	ret
+	pax_ret \name
 
 	ENDPROC(\name)
 .endm
diff --git a/arch/x86/crypto/sha1_ni_asm.S b/arch/x86/crypto/sha1_ni_asm.S
index 874a651..aa3d201 100644
--- a/arch/x86/crypto/sha1_ni_asm.S
+++ b/arch/x86/crypto/sha1_ni_asm.S
@@ -54,6 +54,7 @@
  */
 
 #include <linux/linkage.h>
+#include <asm/alternative-asm.h>
 
 #define DIGEST_PTR	%rdi	/* 1st arg */
 #define DATA_PTR	%rsi	/* 2nd arg */
@@ -290,7 +291,7 @@ ENTRY(sha1_ni_transform)
 .Ldone_hash:
 	mov		RSPSAVE, %rsp
 
-	ret
+	pax_ret sha1_ni_transform
 ENDPROC(sha1_ni_transform)
 
 .data
diff --git a/arch/x86/crypto/sha1_ssse3_asm.S b/arch/x86/crypto/sha1_ssse3_asm.S
index a410950..f0fefc3 100644
--- a/arch/x86/crypto/sha1_ssse3_asm.S
+++ b/arch/x86/crypto/sha1_ssse3_asm.S
@@ -29,6 +29,7 @@
  */
 
 #include <linux/linkage.h>
+#include <asm/alternative-asm.h>
 
 #define CTX	%rdi	// arg1
 #define BUF	%rsi	// arg2
@@ -71,13 +72,14 @@
  * param: function's name
  */
 .macro SHA1_VECTOR_ASM  name
-	ENTRY(\name)
+ALIGN
+	RAP_ENTRY(\name)
 
 	push	%rbx
 	push	%rbp
-	push	%r12
+	push	%r14
 
-	mov	%rsp, %r12
+	mov	%rsp, %r14
 	sub	$64, %rsp		# allocate workspace
 	and	$~15, %rsp		# align stack
 
@@ -99,12 +101,12 @@
 	xor	%rax, %rax
 	rep stosq
 
-	mov	%r12, %rsp		# deallocate workspace
+	mov	%r14, %rsp		# deallocate workspace
 
-	pop	%r12
+	pop	%r14
 	pop	%rbp
 	pop	%rbx
-	ret
+	pax_ret \name
 
 	ENDPROC(\name)
 .endm
diff --git a/arch/x86/crypto/sha256-avx-asm.S b/arch/x86/crypto/sha256-avx-asm.S
index 92b3b5d..47aadd7 100644
--- a/arch/x86/crypto/sha256-avx-asm.S
+++ b/arch/x86/crypto/sha256-avx-asm.S
@@ -49,6 +49,7 @@
 
 #ifdef CONFIG_AS_AVX
 #include <linux/linkage.h>
+#include <asm/alternative-asm.h>
 
 ## assume buffers not aligned
 #define    VMOVDQ vmovdqu
@@ -347,8 +348,7 @@ a = TMP_
 ## arg 3 : Num blocks
 ########################################################################
 .text
-ENTRY(sha256_transform_avx)
-.align 32
+RAP_ENTRY(sha256_transform_avx)
 	pushq   %rbx
 	pushq   %rbp
 	pushq   %r13
@@ -460,7 +460,7 @@ done_hash:
 	popq    %r13
 	popq    %rbp
 	popq    %rbx
-	ret
+	pax_ret sha256_transform_avx
 ENDPROC(sha256_transform_avx)
 
 .data
diff --git a/arch/x86/crypto/sha256-avx2-asm.S b/arch/x86/crypto/sha256-avx2-asm.S
index 570ec5e..6c7f33c 100644
--- a/arch/x86/crypto/sha256-avx2-asm.S
+++ b/arch/x86/crypto/sha256-avx2-asm.S
@@ -50,6 +50,7 @@
 
 #ifdef CONFIG_AS_AVX2
 #include <linux/linkage.h>
+#include <asm/alternative-asm.h>
 
 ## assume buffers not aligned
 #define	VMOVDQ vmovdqu
@@ -528,8 +529,7 @@ STACK_SIZE	= _RSP      + _RSP_SIZE
 ## arg 3 : Num blocks
 ########################################################################
 .text
-ENTRY(sha256_transform_rorx)
-.align 32
+RAP_ENTRY(sha256_transform_rorx)
 	pushq	%rbx
 	pushq	%rbp
 	pushq	%r12
@@ -720,7 +720,7 @@ done_hash:
 	popq	%r12
 	popq	%rbp
 	popq	%rbx
-	ret
+	pax_ret sha256_transform_rorx
 ENDPROC(sha256_transform_rorx)
 
 .data
diff --git a/arch/x86/crypto/sha256-mb/sha256_mb_mgr.h b/arch/x86/crypto/sha256-mb/sha256_mb_mgr.h
index b01ae40..880e1d4 100644
--- a/arch/x86/crypto/sha256-mb/sha256_mb_mgr.h
+++ b/arch/x86/crypto/sha256-mb/sha256_mb_mgr.h
@@ -104,5 +104,6 @@ struct job_sha256 *sha256_mb_mgr_submit_avx2(struct sha256_mb_mgr *state,
 					 struct job_sha256 *job);
 struct job_sha256 *sha256_mb_mgr_flush_avx2(struct sha256_mb_mgr *state);
 struct job_sha256 *sha256_mb_mgr_get_comp_job_avx2(struct sha256_mb_mgr *state);
+struct job_sha256 *sha256_x8_avx2(struct sha256_mb_mgr *state) __rap_hash;
 
 #endif
diff --git a/arch/x86/crypto/sha256-mb/sha256_mb_mgr_flush_avx2.S b/arch/x86/crypto/sha256-mb/sha256_mb_mgr_flush_avx2.S
index a78a069..3919641 100644
--- a/arch/x86/crypto/sha256-mb/sha256_mb_mgr_flush_avx2.S
+++ b/arch/x86/crypto/sha256-mb/sha256_mb_mgr_flush_avx2.S
@@ -52,6 +52,7 @@
  */
 #include <linux/linkage.h>
 #include <asm/frame.h>
+#include <asm/alternative-asm.h>
 #include "sha256_mb_mgr_datastruct.S"
 
 .extern sha256_x8_avx2
@@ -101,7 +102,7 @@ offset = \_offset
 
 # JOB_SHA256* sha256_mb_mgr_flush_avx2(MB_MGR *state)
 # arg 1 : rcx : state
-ENTRY(sha256_mb_mgr_flush_avx2)
+RAP_ENTRY(sha256_mb_mgr_flush_avx2)
 	FRAME_BEGIN
         push    %rbx
 
@@ -181,7 +182,7 @@ LABEL skip_ %I
 
 	# "state" and "args" are the same address, arg1
 	# len is arg2
-	call	sha256_x8_avx2
+	pax_direct_call sha256_x8_avx2
 	# state and idx are intact
 
 len_is_0:
@@ -215,7 +216,7 @@ len_is_0:
 return:
 	pop     %rbx
 	FRAME_END
-	ret
+	pax_ret sha256_mb_mgr_flush_avx2
 
 return_null:
 	xor	job_rax, job_rax
@@ -225,7 +226,7 @@ ENDPROC(sha256_mb_mgr_flush_avx2)
 ##############################################################################
 
 .align 16
-ENTRY(sha256_mb_mgr_get_comp_job_avx2)
+RAP_ENTRY(sha256_mb_mgr_get_comp_job_avx2)
 	push	%rbx
 
 	## if bit 32+3 is set, then all lanes are empty
@@ -276,12 +277,12 @@ ENTRY(sha256_mb_mgr_get_comp_job_avx2)
 
 	pop	%rbx
 
-	ret
+	pax_ret sha256_mb_mgr_get_comp_job_avx2
 
 .return_null:
 	xor	job_rax, job_rax
 	pop	%rbx
-	ret
+	pax_ret sha256_mb_mgr_get_comp_job_avx2
 ENDPROC(sha256_mb_mgr_get_comp_job_avx2)
 
 .data
diff --git a/arch/x86/crypto/sha256-mb/sha256_mb_mgr_submit_avx2.S b/arch/x86/crypto/sha256-mb/sha256_mb_mgr_submit_avx2.S
index 7ea670e..835723c 100644
--- a/arch/x86/crypto/sha256-mb/sha256_mb_mgr_submit_avx2.S
+++ b/arch/x86/crypto/sha256-mb/sha256_mb_mgr_submit_avx2.S
@@ -53,6 +53,7 @@
 
 #include <linux/linkage.h>
 #include <asm/frame.h>
+#include <asm/alternative-asm.h>
 #include "sha256_mb_mgr_datastruct.S"
 
 .extern sha256_x8_avx2
@@ -96,7 +97,7 @@ lane_data	= %r10
 # JOB* sha256_mb_mgr_submit_avx2(MB_MGR *state, JOB_SHA256 *job)
 # arg 1 : rcx : state
 # arg 2 : rdx : job
-ENTRY(sha256_mb_mgr_submit_avx2)
+RAP_ENTRY(sha256_mb_mgr_submit_avx2)
 	FRAME_BEGIN
 	push	%rbx
 	push	%r12
@@ -164,7 +165,7 @@ start_loop:
 
 	# "state" and "args" are the same address, arg1
 	# len is arg2
-	call	sha256_x8_avx2
+	pax_direct_call sha256_x8_avx2
 
 	# state and idx are intact
 
@@ -200,7 +201,7 @@ return:
 	pop     %r12
         pop     %rbx
         FRAME_END
-	ret
+	pax_ret sha256_mb_mgr_submit_avx2
 
 return_null:
 	xor	job_rax, job_rax
diff --git a/arch/x86/crypto/sha256-mb/sha256_x8_avx2.S b/arch/x86/crypto/sha256-mb/sha256_x8_avx2.S
index aa21aea..cb35a6e 100644
--- a/arch/x86/crypto/sha256-mb/sha256_x8_avx2.S
+++ b/arch/x86/crypto/sha256-mb/sha256_x8_avx2.S
@@ -52,6 +52,7 @@
  */
 
 #include <linux/linkage.h>
+#include <asm/alternative-asm.h>
 #include "sha256_mb_mgr_datastruct.S"
 
 ## code to compute oct SHA256 using SSE-256
@@ -435,7 +436,7 @@ Lrounds_16_xx:
 	pop     %r13
 	pop     %r12
 
-	ret
+	pax_ret sha256_x8_avx2
 ENDPROC(sha256_x8_avx2)
 .data
 .align 64
diff --git a/arch/x86/crypto/sha256-ssse3-asm.S b/arch/x86/crypto/sha256-ssse3-asm.S
index 2cedc44..35ed999 100644
--- a/arch/x86/crypto/sha256-ssse3-asm.S
+++ b/arch/x86/crypto/sha256-ssse3-asm.S
@@ -47,6 +47,7 @@
 ########################################################################
 
 #include <linux/linkage.h>
+#include <asm/alternative-asm.h>
 
 ## assume buffers not aligned
 #define    MOVDQ movdqu
@@ -352,9 +353,7 @@ a = TMP_
 ## arg 2 : pointer to input data
 ## arg 3 : Num blocks
 ########################################################################
-.text
-ENTRY(sha256_transform_ssse3)
-.align 32
+RAP_ENTRY(sha256_transform_ssse3)
 	pushq   %rbx
 	pushq   %rbp
 	pushq   %r13
@@ -471,7 +470,7 @@ done_hash:
 	popq    %rbp
 	popq    %rbx
 
-	ret
+	pax_ret sha256_transform_ssse3
 ENDPROC(sha256_transform_ssse3)
 
 .data
diff --git a/arch/x86/crypto/sha256_ni_asm.S b/arch/x86/crypto/sha256_ni_asm.S
index 748cdf2..cd2180d 100644
--- a/arch/x86/crypto/sha256_ni_asm.S
+++ b/arch/x86/crypto/sha256_ni_asm.S
@@ -54,6 +54,7 @@
  */
 
 #include <linux/linkage.h>
+#include <asm/alternative-asm.h>
 
 #define DIGEST_PTR	%rdi	/* 1st arg */
 #define DATA_PTR	%rsi	/* 2nd arg */
@@ -97,7 +98,7 @@
 
 .text
 .align 32
-ENTRY(sha256_ni_transform)
+RAP_ENTRY(sha256_ni_transform)
 
 	shl		$6, NUM_BLKS		/*  convert to bytes */
 	jz		.Ldone_hash
@@ -326,7 +327,7 @@ ENTRY(sha256_ni_transform)
 
 .Ldone_hash:
 
-	ret
+	pax_ret sha256_ni_transform
 ENDPROC(sha256_ni_transform)
 
 .data
diff --git a/arch/x86/crypto/sha512-avx-asm.S b/arch/x86/crypto/sha512-avx-asm.S
index 565274d..106c3dc 100644
--- a/arch/x86/crypto/sha512-avx-asm.S
+++ b/arch/x86/crypto/sha512-avx-asm.S
@@ -49,6 +49,7 @@
 
 #ifdef CONFIG_AS_AVX
 #include <linux/linkage.h>
+#include <asm/alternative-asm.h>
 
 .text
 
@@ -277,7 +278,8 @@ frame_size = frame_GPRSAVE + GPRSAVE_SIZE
 # message blocks.
 # L is the message length in SHA512 blocks
 ########################################################################
-ENTRY(sha512_transform_avx)
+ALIGN
+RAP_ENTRY(sha512_transform_avx)
 	cmp $0, msglen
 	je nowork
 
@@ -364,7 +366,7 @@ updateblock:
 	mov	frame_RSPSAVE(%rsp), %rsp
 
 nowork:
-	ret
+	pax_ret sha512_transform_avx
 ENDPROC(sha512_transform_avx)
 
 ########################################################################
diff --git a/arch/x86/crypto/sha512-avx2-asm.S b/arch/x86/crypto/sha512-avx2-asm.S
index 1f20b35..f12df89 100644
--- a/arch/x86/crypto/sha512-avx2-asm.S
+++ b/arch/x86/crypto/sha512-avx2-asm.S
@@ -51,6 +51,7 @@
 
 #ifdef CONFIG_AS_AVX2
 #include <linux/linkage.h>
+#include <asm/alternative-asm.h>
 
 .text
 
@@ -568,7 +569,8 @@ frame_size = frame_GPRSAVE + GPRSAVE_SIZE
 #   message blocks.
 # L is the message length in SHA512 blocks
 ########################################################################
-ENTRY(sha512_transform_rorx)
+ALIGN
+RAP_ENTRY(sha512_transform_rorx)
 	# Allocate Stack Space
 	mov	%rsp, %rax
 	sub	$frame_size, %rsp
@@ -678,7 +680,7 @@ done_hash:
 
 	# Restore Stack Pointer
 	mov	frame_RSPSAVE(%rsp), %rsp
-	ret
+	pax_ret sha512_transform_rorx
 ENDPROC(sha512_transform_rorx)
 
 ########################################################################
diff --git a/arch/x86/crypto/sha512-mb/sha512_mb_mgr.h b/arch/x86/crypto/sha512-mb/sha512_mb_mgr.h
index 178f17e..88a59c6 100644
--- a/arch/x86/crypto/sha512-mb/sha512_mb_mgr.h
+++ b/arch/x86/crypto/sha512-mb/sha512_mb_mgr.h
@@ -100,5 +100,6 @@ struct job_sha512 *sha512_mb_mgr_submit_avx2(struct sha512_mb_mgr *state,
 						struct job_sha512 *job);
 struct job_sha512 *sha512_mb_mgr_flush_avx2(struct sha512_mb_mgr *state);
 struct job_sha512 *sha512_mb_mgr_get_comp_job_avx2(struct sha512_mb_mgr *state);
+struct job_sha512 *sha512_x4_avx2(struct sha512_mb_mgr *state) __rap_hash;
 
 #endif
diff --git a/arch/x86/crypto/sha512-mb/sha512_mb_mgr_flush_avx2.S b/arch/x86/crypto/sha512-mb/sha512_mb_mgr_flush_avx2.S
index 3ddba19..392d6a1 100644
--- a/arch/x86/crypto/sha512-mb/sha512_mb_mgr_flush_avx2.S
+++ b/arch/x86/crypto/sha512-mb/sha512_mb_mgr_flush_avx2.S
@@ -53,6 +53,7 @@
 
 #include <linux/linkage.h>
 #include <asm/frame.h>
+#include <asm/alternative-asm.h>
 #include "sha512_mb_mgr_datastruct.S"
 
 .extern sha512_x4_avx2
@@ -107,7 +108,7 @@ offset = \_offset
 
 # JOB* sha512_mb_mgr_flush_avx2(MB_MGR *state)
 # arg 1 : rcx : state
-ENTRY(sha512_mb_mgr_flush_avx2)
+RAP_ENTRY(sha512_mb_mgr_flush_avx2)
 	FRAME_BEGIN
 	push	%rbx
 
@@ -177,7 +178,7 @@ LABEL skip_ %I
 
         # "state" and "args" are the same address, arg1
         # len is arg2
-        call    sha512_x4_avx2
+        pax_direct_call sha512_x4_avx2
         # state and idx are intact
 
 len_is_0:
@@ -212,7 +213,7 @@ len_is_0:
 return:
 	pop	%rbx
 	FRAME_END
-        ret
+        pax_ret sha512_mb_mgr_flush_avx2
 
 return_null:
         xor     job_rax, job_rax
@@ -220,7 +221,7 @@ return_null:
 ENDPROC(sha512_mb_mgr_flush_avx2)
 .align 16
 
-ENTRY(sha512_mb_mgr_get_comp_job_avx2)
+RAP_ENTRY(sha512_mb_mgr_get_comp_job_avx2)
         push    %rbx
 
 	mov     _unused_lanes(state), unused_lanes
@@ -273,12 +274,12 @@ ENTRY(sha512_mb_mgr_get_comp_job_avx2)
 
 	pop     %rbx
 
-        ret
+	pax_ret sha512_mb_mgr_get_comp_job_avx2
 
 .return_null:
         xor     job_rax, job_rax
 	pop     %rbx
-        ret
+	pax_ret sha512_mb_mgr_get_comp_job_avx2
 ENDPROC(sha512_mb_mgr_get_comp_job_avx2)
 .data
 
diff --git a/arch/x86/crypto/sha512-mb/sha512_mb_mgr_submit_avx2.S b/arch/x86/crypto/sha512-mb/sha512_mb_mgr_submit_avx2.S
index 815f07b..a1f961a 100644
--- a/arch/x86/crypto/sha512-mb/sha512_mb_mgr_submit_avx2.S
+++ b/arch/x86/crypto/sha512-mb/sha512_mb_mgr_submit_avx2.S
@@ -53,6 +53,7 @@
 
 #include <linux/linkage.h>
 #include <asm/frame.h>
+#include <asm/alternative-asm.h>
 #include "sha512_mb_mgr_datastruct.S"
 
 .extern sha512_x4_avx2
@@ -98,7 +99,7 @@
 # JOB* sha512_mb_mgr_submit_avx2(MB_MGR *state, JOB *job)
 # arg 1 : rcx : state
 # arg 2 : rdx : job
-ENTRY(sha512_mb_mgr_submit_avx2)
+RAP_ENTRY(sha512_mb_mgr_submit_avx2)
 	FRAME_BEGIN
 	push	%rbx
 	push	%r12
@@ -167,7 +168,7 @@ start_loop:
 
 	# "state" and "args" are the same address, arg1
 	# len is arg2
-	call    sha512_x4_avx2
+	pax_direct_call sha512_x4_avx2
 	# state and idx are intact
 
 len_is_0:
@@ -203,7 +204,7 @@ return:
 	pop	%r12
 	pop	%rbx
 	FRAME_END
-	ret
+	pax_ret sha512_mb_mgr_submit_avx2
 
 return_null:
 	xor     job_rax, job_rax
diff --git a/arch/x86/crypto/sha512-mb/sha512_x4_avx2.S b/arch/x86/crypto/sha512-mb/sha512_x4_avx2.S
index 31ab1ef..da5a002 100644
--- a/arch/x86/crypto/sha512-mb/sha512_x4_avx2.S
+++ b/arch/x86/crypto/sha512-mb/sha512_x4_avx2.S
@@ -63,6 +63,7 @@
 # clobbers ymm0-15
 
 #include <linux/linkage.h>
+#include <asm/alternative-asm.h>
 #include "sha512_mb_mgr_datastruct.S"
 
 arg1 = %rdi
@@ -358,7 +359,7 @@ Lrounds_16_xx:
 	pop     %r12
 
 	# outer calling routine restores XMM and other GP registers
-	ret
+	pax_ret sha512_x4_avx2
 ENDPROC(sha512_x4_avx2)
 
 .data
diff --git a/arch/x86/crypto/sha512-ssse3-asm.S b/arch/x86/crypto/sha512-ssse3-asm.S
index e610e29..6b3848e 100644
--- a/arch/x86/crypto/sha512-ssse3-asm.S
+++ b/arch/x86/crypto/sha512-ssse3-asm.S
@@ -48,6 +48,7 @@
 ########################################################################
 
 #include <linux/linkage.h>
+#include <asm/alternative-asm.h>
 
 .text
 
@@ -275,7 +276,8 @@ frame_size = frame_GPRSAVE + GPRSAVE_SIZE
 #   message blocks.
 # L is the message length in SHA512 blocks.
 ########################################################################
-ENTRY(sha512_transform_ssse3)
+ALIGN
+RAP_ENTRY(sha512_transform_ssse3)
 
 	cmp $0, msglen
 	je nowork
@@ -363,7 +365,7 @@ updateblock:
 	mov	frame_RSPSAVE(%rsp), %rsp
 
 nowork:
-	ret
+	pax_ret sha512_transform_ssse3
 ENDPROC(sha512_transform_ssse3)
 
 ########################################################################
diff --git a/arch/x86/crypto/twofish-avx-x86_64-asm_64.S b/arch/x86/crypto/twofish-avx-x86_64-asm_64.S
index dc66273..91dc734b 100644
--- a/arch/x86/crypto/twofish-avx-x86_64-asm_64.S
+++ b/arch/x86/crypto/twofish-avx-x86_64-asm_64.S
@@ -25,6 +25,7 @@
 
 #include <linux/linkage.h>
 #include <asm/frame.h>
+#include <asm/alternative-asm.h>
 #include "glue_helper-asm-avx.S"
 
 .file "twofish-avx-x86_64-asm_64.S"
@@ -285,7 +286,7 @@ __twofish_enc_blk8:
 	outunpack_blocks(RC1, RD1, RA1, RB1, RK1, RX0, RY0, RK2);
 	outunpack_blocks(RC2, RD2, RA2, RB2, RK1, RX0, RY0, RK2);
 
-	ret;
+	pax_ret __twofish_enc_blk8;
 ENDPROC(__twofish_enc_blk8)
 
 .align 8
@@ -325,10 +326,10 @@ __twofish_dec_blk8:
 	outunpack_blocks(RA1, RB1, RC1, RD1, RK1, RX0, RY0, RK2);
 	outunpack_blocks(RA2, RB2, RC2, RD2, RK1, RX0, RY0, RK2);
 
-	ret;
+	pax_ret __twofish_dec_blk8;
 ENDPROC(__twofish_dec_blk8)
 
-ENTRY(twofish_ecb_enc_8way)
+RAP_ENTRY(twofish_ecb_enc_8way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst
@@ -340,15 +341,15 @@ ENTRY(twofish_ecb_enc_8way)
 
 	load_8way(%rdx, RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2);
 
-	call __twofish_enc_blk8;
+	pax_direct_call __twofish_enc_blk8;
 
 	store_8way(%r11, RC1, RD1, RA1, RB1, RC2, RD2, RA2, RB2);
 
 	FRAME_END
-	ret;
+	pax_ret twofish_ecb_enc_8way;
 ENDPROC(twofish_ecb_enc_8way)
 
-ENTRY(twofish_ecb_dec_8way)
+RAP_ENTRY(twofish_ecb_dec_8way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst
@@ -360,15 +361,15 @@ ENTRY(twofish_ecb_dec_8way)
 
 	load_8way(%rdx, RC1, RD1, RA1, RB1, RC2, RD2, RA2, RB2);
 
-	call __twofish_dec_blk8;
+	pax_direct_call __twofish_dec_blk8;
 
 	store_8way(%r11, RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2);
 
 	FRAME_END
-	ret;
+	pax_ret twofish_ecb_dec_8way;
 ENDPROC(twofish_ecb_dec_8way)
 
-ENTRY(twofish_cbc_dec_8way)
+RAP_ENTRY(twofish_cbc_dec_8way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst
@@ -376,24 +377,24 @@ ENTRY(twofish_cbc_dec_8way)
 	 */
 	FRAME_BEGIN
 
-	pushq %r12;
+	pushq %r14;
 
 	movq %rsi, %r11;
-	movq %rdx, %r12;
+	movq %rdx, %r14;
 
 	load_8way(%rdx, RC1, RD1, RA1, RB1, RC2, RD2, RA2, RB2);
 
-	call __twofish_dec_blk8;
+	pax_direct_call __twofish_dec_blk8;
 
-	store_cbc_8way(%r12, %r11, RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2);
+	store_cbc_8way(%r14, %r11, RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2);
 
-	popq %r12;
+	popq %r14;
 
 	FRAME_END
-	ret;
+	pax_ret twofish_cbc_dec_8way;
 ENDPROC(twofish_cbc_dec_8way)
 
-ENTRY(twofish_ctr_8way)
+RAP_ENTRY(twofish_ctr_8way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst
@@ -402,25 +403,25 @@ ENTRY(twofish_ctr_8way)
 	 */
 	FRAME_BEGIN
 
-	pushq %r12;
+	pushq %r14;
 
 	movq %rsi, %r11;
-	movq %rdx, %r12;
+	movq %rdx, %r14;
 
 	load_ctr_8way(%rcx, .Lbswap128_mask, RA1, RB1, RC1, RD1, RA2, RB2, RC2,
 		      RD2, RX0, RX1, RY0);
 
-	call __twofish_enc_blk8;
+	pax_direct_call __twofish_enc_blk8;
 
-	store_ctr_8way(%r12, %r11, RC1, RD1, RA1, RB1, RC2, RD2, RA2, RB2);
+	store_ctr_8way(%r14, %r11, RC1, RD1, RA1, RB1, RC2, RD2, RA2, RB2);
 
-	popq %r12;
+	popq %r14;
 
 	FRAME_END
-	ret;
+	pax_ret twofish_ctr_8way;
 ENDPROC(twofish_ctr_8way)
 
-ENTRY(twofish_xts_enc_8way)
+RAP_ENTRY(twofish_xts_enc_8way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst
@@ -435,16 +436,16 @@ ENTRY(twofish_xts_enc_8way)
 	load_xts_8way(%rcx, %rdx, %rsi, RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2,
 		      RX0, RX1, RY0, .Lxts_gf128mul_and_shl1_mask);
 
-	call __twofish_enc_blk8;
+	pax_direct_call __twofish_enc_blk8;
 
 	/* dst <= regs xor IVs(in dst) */
 	store_xts_8way(%r11, RC1, RD1, RA1, RB1, RC2, RD2, RA2, RB2);
 
 	FRAME_END
-	ret;
+	pax_ret twofish_xts_enc_8way;
 ENDPROC(twofish_xts_enc_8way)
 
-ENTRY(twofish_xts_dec_8way)
+RAP_ENTRY(twofish_xts_dec_8way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst
@@ -459,11 +460,11 @@ ENTRY(twofish_xts_dec_8way)
 	load_xts_8way(%rcx, %rdx, %rsi, RC1, RD1, RA1, RB1, RC2, RD2, RA2, RB2,
 		      RX0, RX1, RY0, .Lxts_gf128mul_and_shl1_mask);
 
-	call __twofish_dec_blk8;
+	pax_direct_call __twofish_dec_blk8;
 
 	/* dst <= regs xor IVs(in dst) */
 	store_xts_8way(%r11, RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2);
 
 	FRAME_END
-	ret;
+	pax_ret twofish_xts_dec_8way;
 ENDPROC(twofish_xts_dec_8way)
diff --git a/arch/x86/crypto/twofish-i586-asm_32.S b/arch/x86/crypto/twofish-i586-asm_32.S
index 694ea45..91b9a8d 100644
--- a/arch/x86/crypto/twofish-i586-asm_32.S
+++ b/arch/x86/crypto/twofish-i586-asm_32.S
@@ -22,6 +22,7 @@
 
 #include <linux/linkage.h>
 #include <asm/asm-offsets.h>
+#include <asm/alternative-asm.h>
 
 /* return address at 0 */
 
@@ -220,7 +221,7 @@
 	xor	%esi,		d ## D;\
 	ror	$1,		d ## D;
 
-ENTRY(twofish_enc_blk)
+RAP_ENTRY(twofish_enc_blk)
 	push	%ebp			/* save registers according to calling convention*/
 	push    %ebx
 	push    %esi
@@ -273,10 +274,10 @@ ENTRY(twofish_enc_blk)
 	pop	%ebx
 	pop	%ebp
 	mov	$1,	%eax
-	ret
+	pax_ret twofish_enc_blk
 ENDPROC(twofish_enc_blk)
 
-ENTRY(twofish_dec_blk)
+RAP_ENTRY(twofish_dec_blk)
 	push	%ebp			/* save registers according to calling convention*/
 	push    %ebx
 	push    %esi
@@ -330,5 +331,5 @@ ENTRY(twofish_dec_blk)
 	pop	%ebx
 	pop	%ebp
 	mov	$1,	%eax
-	ret
+	pax_ret twofish_dec_blk
 ENDPROC(twofish_dec_blk)
diff --git a/arch/x86/crypto/twofish-x86_64-asm_64-3way.S b/arch/x86/crypto/twofish-x86_64-asm_64-3way.S
index 1c3b7ce..9a65a0b 100644
--- a/arch/x86/crypto/twofish-x86_64-asm_64-3way.S
+++ b/arch/x86/crypto/twofish-x86_64-asm_64-3way.S
@@ -21,6 +21,7 @@
  */
 
 #include <linux/linkage.h>
+#include <asm/alternative-asm.h>
 
 .file "twofish-x86_64-asm-3way.S"
 .text
@@ -258,7 +259,7 @@ ENTRY(__twofish_enc_blk_3way)
 	popq %r13;
 	popq %r14;
 	popq %r15;
-	ret;
+	pax_ret __twofish_enc_blk_3way;
 
 .L__enc_xor3:
 	outunpack_enc3(xor);
@@ -269,10 +270,10 @@ ENTRY(__twofish_enc_blk_3way)
 	popq %r13;
 	popq %r14;
 	popq %r15;
-	ret;
+	pax_ret __twofish_enc_blk_3way;
 ENDPROC(__twofish_enc_blk_3way)
 
-ENTRY(twofish_dec_blk_3way)
+RAP_ENTRY(twofish_dec_blk_3way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst
@@ -308,5 +309,5 @@ ENTRY(twofish_dec_blk_3way)
 	popq %r13;
 	popq %r14;
 	popq %r15;
-	ret;
+	pax_ret twofish_dec_blk_3way;
 ENDPROC(twofish_dec_blk_3way)
diff --git a/arch/x86/crypto/twofish-x86_64-asm_64.S b/arch/x86/crypto/twofish-x86_64-asm_64.S
index a350c99..b59af9f 100644
--- a/arch/x86/crypto/twofish-x86_64-asm_64.S
+++ b/arch/x86/crypto/twofish-x86_64-asm_64.S
@@ -22,6 +22,7 @@
 
 #include <linux/linkage.h>
 #include <asm/asm-offsets.h>
+#include <asm/alternative-asm.h>
 
 #define a_offset	0
 #define b_offset	4
@@ -215,7 +216,7 @@
 	xor	%r8d,		d ## D;\
 	ror	$1,		d ## D;
 
-ENTRY(twofish_enc_blk)
+RAP_ENTRY(twofish_enc_blk)
 	pushq    R1
 
 	/* %rdi contains the ctx address */
@@ -265,10 +266,10 @@ ENTRY(twofish_enc_blk)
 
 	popq	R1
 	movl	$1,%eax
-	ret
+	pax_ret twofish_enc_blk
 ENDPROC(twofish_enc_blk)
 
-ENTRY(twofish_dec_blk)
+RAP_ENTRY(twofish_dec_blk)
 	pushq    R1
 
 	/* %rdi contains the ctx address */
@@ -317,5 +318,5 @@ ENTRY(twofish_dec_blk)
 
 	popq	R1
 	movl	$1,%eax
-	ret
+	pax_ret twofish_dec_blk
 ENDPROC(twofish_dec_blk)
diff --git a/arch/x86/entry/calling.h b/arch/x86/entry/calling.h
index 9a9e588..4f8115a 100644
--- a/arch/x86/entry/calling.h
+++ b/arch/x86/entry/calling.h
@@ -212,7 +226,7 @@ For 32-bit we have the following conventions - kernel is built with
 #ifdef HAVE_JUMP_LABEL
 	STATIC_JUMP_IF_FALSE .Lafter_call_\@, context_tracking_enabled, def=0
 #endif
-	call enter_from_user_mode
+	pax_direct_call enter_from_user_mode
 .Lafter_call_\@:
 #endif
 .endm
diff --git a/arch/x86/entry/entry_32.S b/arch/x86/entry/entry_32.S
index edba860..d684e0f 100644
--- a/arch/x86/entry/entry_32.S
+++ b/arch/x86/entry/entry_32.S
@@ -235,7 +389,7 @@ ENTRY(__switch_to_asm)
 	popl	%ebp
 
 	jmp	__switch_to
-END(__switch_to_asm)
+ENDPROC(__switch_to_asm)
 
 /*
  * A newly forked process directly context switches into this address.
@@ -246,7 +400,7 @@ END(__switch_to_asm)
  */
 ENTRY(ret_from_fork)
 	pushl	%eax
-	call	schedule_tail
+	pax_direct_call schedule_tail
 	popl	%eax
 
 	testl	%ebx, %ebx
@@ -255,12 +409,12 @@ ENTRY(ret_from_fork)
 2:
 	/* When we fork, we trace the syscall return in the child, too. */
 	movl    %esp, %eax
-	call    syscall_return_slowpath
+	pax_direct_call syscall_return_slowpath
 	jmp     restore_all
 
 	/* kernel thread */
 1:	movl	%edi, %eax
-	call	*%ebx
+	pax_indirect_call "%ebx", kthreadd
 	/*
 	 * A kernel thread is allowed to return here after successfully
 	 * calling do_execve().  Exit to userspace to complete the execve()
@@ -268,7 +422,7 @@ ENTRY(ret_from_fork)
 	 */
 	movl	$0, PT_EAX(%esp)
 	jmp	2b
-END(ret_from_fork)
+ENDPROC(ret_from_fork)
 
 /*
  * Return to user mode is not as complex as all this looks,
@@ -300,9 +454,9 @@ ret_from_intr:
 	DISABLE_INTERRUPTS(CLBR_ANY)
 	TRACE_IRQS_OFF
 	movl	%esp, %eax
-	call	prepare_exit_to_usermode
-	jmp	restore_all
-END(ret_from_exception)
+	pax_direct_call prepare_exit_to_usermode
+	jmp	.Lsyscall_32_done
+ENDPROC(ret_from_exception)
 
 #ifdef CONFIG_PREEMPT
 ENTRY(resume_kernel)
@@ -312,9 +474,9 @@ need_resched:
 	jnz	restore_all
 	testl	$X86_EFLAGS_IF, PT_EFLAGS(%esp)	# interrupts off (exception path) ?
 	jz	restore_all
-	call	preempt_schedule_irq
+	pax_direct_call preempt_schedule_irq
 	jmp	need_resched
-END(resume_kernel)
+ENDPROC(resume_kernel)
 #endif
 
 GLOBAL(__begin_SYSENTER_singlestep_region)
@@ -411,7 +577,7 @@ sysenter_past_esp:
 	TRACE_IRQS_OFF
 
 	movl	%esp, %eax
-	call	do_fast_syscall_32
+	pax_direct_call do_fast_syscall_32
 	/* XEN PV guests always use IRET path */
 	ALTERNATIVE "testl %eax, %eax; jz .Lsyscall_32_done", \
 		    "jmp .Lsyscall_32_done", X86_FEATURE_XENPV
@@ -499,6 +684,6 @@ ENTRY(entry_INT80_32)
 	TRACE_IRQS_OFF
 
 	movl	%esp, %eax
-	call	do_int80_syscall_32
+	pax_direct_call do_int80_syscall_32
 .Lsyscall_32_done:
 
@@ -613,7 +832,7 @@ ENTRY(irq_entries_start)
 	jmp	common_interrupt
 	.align	8
     .endr
-END(irq_entries_start)
+ENDPROC(irq_entries_start)
 
 /*
  * the CPU automatically disables interrupts when executing an IRQ vector,
@@ -626,7 +845,7 @@ common_interrupt:
 	SAVE_ALL
 	TRACE_IRQS_OFF
 	movl	%esp, %eax
-	call	do_IRQ
+	pax_direct_call do_IRQ
 	jmp	ret_from_intr
 ENDPROC(common_interrupt)
 
@@ -637,7 +856,7 @@ ENTRY(name)				\
 	SAVE_ALL;			\
 	TRACE_IRQS_OFF			\
 	movl	%esp, %eax;		\
-	call	fn;			\
+	pax_direct_call fn;		\
 	jmp	ret_from_intr;		\
 ENDPROC(name)
 
@@ -660,7 +879,7 @@ ENTRY(coprocessor_error)
 	pushl	$0
 	pushl	$do_coprocessor_error
 	jmp	error_code
-END(coprocessor_error)
+ENDPROC(coprocessor_error)
 
 ENTRY(simd_coprocessor_error)
 	ASM_CLAC
@@ -674,20 +893,20 @@ ENTRY(simd_coprocessor_error)
 	pushl	$do_simd_coprocessor_error
 #endif
 	jmp	error_code
-END(simd_coprocessor_error)
+ENDPROC(simd_coprocessor_error)
 
 ENTRY(device_not_available)
 	ASM_CLAC
 	pushl	$-1				# mark this as an int
 	pushl	$do_device_not_available
 	jmp	error_code
-END(device_not_available)
+ENDPROC(device_not_available)
 
 #ifdef CONFIG_PARAVIRT
 ENTRY(native_iret)
 	iret
 	_ASM_EXTABLE(native_iret, iret_exc)
-END(native_iret)
+ENDPROC(native_iret)
 #endif
 
 ENTRY(overflow)
@@ -695,59 +914,59 @@ ENTRY(overflow)
 	pushl	$0
 	pushl	$do_overflow
 	jmp	error_code
-END(overflow)
+ENDPROC(overflow)
 
 ENTRY(bounds)
 	ASM_CLAC
 	pushl	$0
 	pushl	$do_bounds
 	jmp	error_code
-END(bounds)
+ENDPROC(bounds)
 
 ENTRY(invalid_op)
 	ASM_CLAC
 	pushl	$0
 	pushl	$do_invalid_op
 	jmp	error_code
-END(invalid_op)
+ENDPROC(invalid_op)
 
 ENTRY(coprocessor_segment_overrun)
 	ASM_CLAC
 	pushl	$0
 	pushl	$do_coprocessor_segment_overrun
 	jmp	error_code
-END(coprocessor_segment_overrun)
+ENDPROC(coprocessor_segment_overrun)
 
 ENTRY(invalid_TSS)
 	ASM_CLAC
 	pushl	$do_invalid_TSS
 	jmp	error_code
-END(invalid_TSS)
+ENDPROC(invalid_TSS)
 
 ENTRY(segment_not_present)
 	ASM_CLAC
 	pushl	$do_segment_not_present
 	jmp	error_code
-END(segment_not_present)
+ENDPROC(segment_not_present)
 
 ENTRY(stack_segment)
 	ASM_CLAC
 	pushl	$do_stack_segment
 	jmp	error_code
-END(stack_segment)
+ENDPROC(stack_segment)
 
 ENTRY(alignment_check)
 	ASM_CLAC
 	pushl	$do_alignment_check
 	jmp	error_code
-END(alignment_check)
+ENDPROC(alignment_check)
 
 ENTRY(divide_error)
 	ASM_CLAC
 	pushl	$0				# no error code
 	pushl	$do_divide_error
 	jmp	error_code
-END(divide_error)
+ENDPROC(divide_error)
 
 #ifdef CONFIG_X86_MCE
 ENTRY(machine_check)
@@ -755,7 +974,7 @@ ENTRY(machine_check)
 	pushl	$0
 	pushl	machine_check_vector
 	jmp	error_code
-END(machine_check)
+ENDPROC(machine_check)
 #endif
 
 ENTRY(spurious_interrupt_bug)
@@ -763,5 +982,5 @@ ENTRY(spurious_interrupt_bug)
 	pushl	$0
 	pushl	$do_spurious_interrupt_bug
 	jmp	error_code
-END(spurious_interrupt_bug)
+ENDPROC(spurious_interrupt_bug)
 
@@ -788,9 +1032,9 @@ ENTRY(xen_hypervisor_callback)
 
 ENTRY(xen_do_upcall)
 1:	mov	%esp, %eax
-	call	xen_evtchn_do_upcall
+	pax_direct_call xen_evtchn_do_upcall
 #ifndef CONFIG_PREEMPT
-	call	xen_maybe_preempt_hcall
+	pax_direct_call xen_maybe_preempt_hcall
 #endif
 	jmp	ret_from_intr
 ENDPROC(xen_hypervisor_callback)
@@ -861,8 +1105,8 @@ BUILD_INTERRUPT3(hyperv_callback_vector, HYPERVISOR_CALLBACK_VECTOR,
 #ifdef CONFIG_DYNAMIC_FTRACE
 
 ENTRY(mcount)
-	ret
-END(mcount)
+	pax_ret mcount
+ENDPROC(mcount)
 
 ENTRY(ftrace_caller)
 	pushl	%eax
@@ -876,7 +1120,7 @@ ENTRY(ftrace_caller)
 
 .globl ftrace_call
 ftrace_call:
-	call	ftrace_stub
+	pax_direct_call ftrace_stub
 
 	addl	$4, %esp			/* skip NULL pointer */
 	popl	%edx
@@ -891,8 +1135,8 @@ ftrace_graph_call:
 
 /* This is weak to keep gas from relaxing the jumps */
 WEAK(ftrace_stub)
-	ret
-END(ftrace_caller)
+	pax_ret ftrace_caller
+ENDPROC(ftrace_caller)
 
 ENTRY(ftrace_regs_caller)
 	pushf	/* push flags before compare (in cs location) */
@@ -931,7 +1175,7 @@ ENTRY(ftrace_regs_caller)
 	pushl	%esp				/* Save pt_regs as 4th parameter */
 
 GLOBAL(ftrace_regs_call)
-	call	ftrace_stub
+	pax_direct_call ftrace_stub
 
 	addl	$4, %esp			/* Skip pt_regs */
 	movl	14*4(%esp), %eax		/* Move flags back into cs */
@@ -973,7 +1217,7 @@ ENTRY(mcount)
 #endif
 .globl ftrace_stub
 ftrace_stub:
-	ret
+	pax_ret ftrace_stub
 
 	/* taken from glibc */
 trace:
@@ -984,13 +1228,13 @@ trace:
 	movl	0x4(%ebp), %edx
 	subl	$MCOUNT_INSN_SIZE, %eax
 
-	call	*ftrace_trace_function
+	pax_indirect_call "ftrace_trace_function", ftrace_stub
 
 	popl	%edx
 	popl	%ecx
 	popl	%eax
 	jmp	ftrace_stub
-END(mcount)
+ENDPROC(mcount)
 #endif /* CONFIG_DYNAMIC_FTRACE */
 EXPORT_SYMBOL(mcount)
 #endif /* CONFIG_FUNCTION_TRACER */
@@ -1004,19 +1248,19 @@ ENTRY(ftrace_graph_caller)
 	lea	0x4(%ebp), %edx
 	movl	(%ebp), %ecx
 	subl	$MCOUNT_INSN_SIZE, %eax
-	call	prepare_ftrace_return
+	pax_direct_call prepare_ftrace_return
 	popl	%edx
 	popl	%ecx
 	popl	%eax
-	ret
-END(ftrace_graph_caller)
+	pax_ret ftrace_graph_caller
+ENDPROC(ftrace_graph_caller)
 
 .globl return_to_handler
 return_to_handler:
 	pushl	%eax
 	pushl	%edx
 	movl	%ebp, %eax
-	call	ftrace_return_to_handler
+	pax_direct_call ftrace_return_to_handler
 	movl	%eax, %ecx
 	popl	%edx
 	popl	%eax
@@ -1028,7 +1272,7 @@ ENTRY(trace_page_fault)
 	ASM_CLAC
 	pushl	$trace_do_page_fault
 	jmp	error_code
-END(trace_page_fault)
+ENDPROC(trace_page_fault)
 #endif
 
 ENTRY(page_fault)
@@ -1175,21 +1443,21 @@ ENTRY(int3)
 	TRACE_IRQS_OFF
 	xorl	%edx, %edx			# zero error code
 	movl	%esp, %eax			# pt_regs pointer
-	call	do_int3
+	pax_direct_call do_int3
 	jmp	ret_from_exception
-END(int3)
+ENDPROC(int3)
 
 ENTRY(general_protection)
 	pushl	$do_general_protection
 	jmp	error_code
-END(general_protection)
+ENDPROC(general_protection)
 
 #ifdef CONFIG_KVM_GUEST
 ENTRY(async_page_fault)
 	ASM_CLAC
 	pushl	$do_async_page_fault
 	jmp	error_code
-END(async_page_fault)
+ENDPROC(async_page_fault)
 #endif
 
 ENTRY(rewind_stack_do_exit)
@@ -1199,6 +1467,6 @@ ENTRY(rewind_stack_do_exit)
 	movl	PER_CPU_VAR(cpu_current_top_of_stack), %esi
 	leal	-TOP_OF_KERNEL_STACK_PADDING-PTREGS_SIZE(%esi), %esp
 
-	call	do_exit
+	pax_direct_call do_group_exit
 1:	jmp 1b
-END(rewind_stack_do_exit)
+ENDPROC(rewind_stack_do_exit)
diff --git a/arch/x86/entry/entry_64.S b/arch/x86/entry/entry_64.S
index ef766a3..d3f0e59 100644
--- a/arch/x86/entry/entry_64.S
+++ b/arch/x86/entry/entry_64.S
@@ -77,15 +466,15 @@ ENDPROC(native_usergs_sysret64)
 #if defined(CONFIG_DYNAMIC_FTRACE) && defined(CONFIG_TRACE_IRQFLAGS)
 
 .macro TRACE_IRQS_OFF_DEBUG
-	call	debug_stack_set_zero
+	pax_direct_call debug_stack_set_zero
 	TRACE_IRQS_OFF
-	call	debug_stack_reset
+	pax_direct_call debug_stack_reset
 .endm
 
 .macro TRACE_IRQS_ON_DEBUG
-	call	debug_stack_set_zero
+	pax_direct_call debug_stack_set_zero
 	TRACE_IRQS_ON
-	call	debug_stack_reset
+	pax_direct_call debug_stack_reset
 .endm
 
 .macro TRACE_IRQS_IRETQ_DEBUG
@@ -206,7 +605,7 @@ entry_SYSCALL_64_fastpath:
 	 * It might end up jumping to the slow path.  If it jumps, RAX
 	 * and all argument registers are clobbered.
 	 */
-	call	*sys_call_table(, %rax, 8)
+	pax_indirect_call "sys_call_table(, %rax, 8)", sys_ni_syscall
 .Lentry_SYSCALL_64_after_fastpath_call:
 
 	movq	%rax, RAX(%rsp)
@@ -241,13 +643,13 @@ entry_SYSCALL_64_fastpath:
 	ENABLE_INTERRUPTS(CLBR_NONE)
 	SAVE_EXTRA_REGS
 	movq	%rsp, %rdi
-	call	syscall_return_slowpath	/* returns with IRQs disabled */
+	pax_direct_call syscall_return_slowpath	/* returns with IRQs disabled */
 	jmp	return_from_SYSCALL_64
 
 entry_SYSCALL64_slow_path:
 	/* IRQs are off. */
 	SAVE_EXTRA_REGS
 	movq	%rsp, %rdi
-	call	do_syscall_64		/* returns with IRQs disabled */
+	pax_direct_call do_syscall_64		/* returns with IRQs disabled */
 
 return_from_SYSCALL_64:
@@ -329,7 +733,7 @@ syscall_return_via_sysret:
 opportunistic_sysret_failed:
 	SWAPGS
 	jmp	restore_c_regs_and_iret
-END(entry_SYSCALL_64)
+ENDPROC(entry_SYSCALL_64)
 
 ENTRY(stub_ptregs_64)
 	/*
@@ -355,13 +759,17 @@ ENTRY(stub_ptregs_64)
 
 1:
 	jmp	*%rax				/* Called from C */
-END(stub_ptregs_64)
+ENDPROC(stub_ptregs_64)
 
 .macro ptregs_stub func
-ENTRY(ptregs_\func)
+RAP_ENTRY(ptregs_\func)
+#ifdef CONFIG_PAX_RAP
+	leaq	rap_\func(%rip), %rax
+#else
 	leaq	\func(%rip), %rax
+#endif
 	jmp	stub_ptregs_64
-END(ptregs_\func)
+ENDPROC(ptregs_\func)
 .endm
 
 /* Instantiate ptregs_stub for each ptregs-using syscall */
@@ -403,7 +813,7 @@ ENTRY(__switch_to_asm)
 	popq	%rbp
 
 	jmp	__switch_to
-END(__switch_to_asm)
+ENDPROC(__switch_to_asm)
 
 /*
  * A newly forked process directly context switches into this address.
@@ -438,7 +859,7 @@ ENTRY(ret_from_fork)
 	 */
 	movq	$0, RAX(%rsp)
 	jmp	2b
-END(ret_from_fork)
+ENDPROC(ret_from_fork)
 
 /*
  * Build the entry stubs with some assembler magic.
@@ -453,7 +874,7 @@ ENTRY(irq_entries_start)
 	jmp	common_interrupt
 	.align	8
     .endr
-END(irq_entries_start)
+ENDPROC(irq_entries_start)
 
 /*
  * Interrupt entry/exit.
@@ -506,7 +935,7 @@ END(irq_entries_start)
 	/* We entered an interrupt context - irqs are off: */
 	TRACE_IRQS_OFF
 
-	call	\func	/* rdi points to pt_regs */
+	pax_direct_call \func	/* rdi points to pt_regs */
 	.endm
 
 	/*
@@ -654,7 +1101,7 @@ native_irq_return_ldt:
 	 */
 	jmp	native_irq_return_iret
 #endif
-END(common_interrupt)
+ENDPROC(common_interrupt)
 
 /*
  * APIC interrupts.
@@ -666,7 +1113,7 @@ ENTRY(\sym)
 .Lcommon_\sym:
 	interrupt \do_sym
 	jmp	ret_from_intr
-END(\sym)
+ENDPROC(\sym)
 .endm
 
 #ifdef CONFIG_TRACING
@@ -765,9 +1216,9 @@ ENTRY(\sym)
 	testb	$3, CS(%rsp)			/* If coming from userspace, switch stacks */
 	jnz	1f
 	.endif
-	call	paranoid_entry
+	pax_direct_call paranoid_entry
 	.else
-	call	error_entry
+	pax_direct_call error_entry
 	.endif
 	/* returned flag: ebx=0: need swapgs on exit, ebx=1: don't need it */
 
@@ -812,11 +1276,11 @@ ENTRY(\sym)
 	 * run in real process context if user_mode(regs).
 	 */
 1:
-	call	error_entry
+	pax_direct_call error_entry
 
 
 	movq	%rsp, %rdi			/* pt_regs pointer */
-	call	sync_regs
+	pax_direct_call sync_regs
 	movq	%rax, %rsp			/* switch stack */
 
 	movq	%rsp, %rdi			/* pt_regs pointer */
@@ -874,8 +1350,8 @@ ENTRY(native_load_gs_index)
 2:	ALTERNATIVE "", "mfence", X86_BUG_SWAPGS_FENCE
 	SWAPGS
 	popfq
-	ret
-END(native_load_gs_index)
+	pax_ret native_load_gs_index
+ENDPROC(native_load_gs_index)
 EXPORT_SYMBOL(native_load_gs_index)
 
 	_ASM_EXTABLE(.Lgs_change, bad_gs)
@@ -901,14 +1377,14 @@ ENTRY(do_softirq_own_stack)
 	incl	PER_CPU_VAR(irq_count)
 	cmove	PER_CPU_VAR(irq_stack_ptr), %rsp
 	push	%rbp				/* frame pointer backlink */
-	call	__do_softirq
+	pax_direct_call __do_softirq
 	leaveq
 	decl	PER_CPU_VAR(irq_count)
-	ret
-END(do_softirq_own_stack)
+	pax_ret do_softirq_own_stack
+ENDPROC(do_softirq_own_stack)
 
 #ifdef CONFIG_XEN
-idtentry xen_hypervisor_callback xen_do_hypervisor_callback has_error_code=0
+idtentry xen_hypervisor_callback xen_do_hypervisor_callback has_error_code=0 rap_hash=tailcall
 
 /*
  * A note on the "critical region" in our callback handler.
@@ -929,19 +1405,18 @@ ENTRY(xen_do_hypervisor_callback)		/* do_hypervisor_callback(struct *pt_regs) */
  * Since we don't modify %rdi, evtchn_do_upall(struct *pt_regs) will
  * see the correct pointer to the pt_regs
  */
-	movq	%rdi, %rsp			/* we don't return, adjust the stack frame */
 11:	incl	PER_CPU_VAR(irq_count)
 	movq	%rsp, %rbp
 	cmovzq	PER_CPU_VAR(irq_stack_ptr), %rsp
 	pushq	%rbp				/* frame pointer backlink */
-	call	xen_evtchn_do_upcall
+	pax_direct_call xen_evtchn_do_upcall
 	popq	%rsp
 	decl	PER_CPU_VAR(irq_count)
 #ifndef CONFIG_PREEMPT
-	call	xen_maybe_preempt_hcall
+	pax_direct_call xen_maybe_preempt_hcall
 #endif
 	jmp	error_exit
-END(xen_do_hypervisor_callback)
+ENDPROC(xen_do_hypervisor_callback)
 
 /*
  * Hypervisor uses this for application faults while it executes.
@@ -986,7 +1461,7 @@ ENTRY(xen_failsafe_callback)
 	SAVE_C_REGS
 	SAVE_EXTRA_REGS
 	jmp	error_exit
-END(xen_failsafe_callback)
+ENDPROC(xen_failsafe_callback)
 
 apicinterrupt3 HYPERVISOR_CALLBACK_VECTOR \
 	xen_hvm_callback_vector xen_evtchn_do_upcall
@@ -1016,7 +1491,7 @@ idtentry async_page_fault	do_async_page_fault	has_error_code=1
 #endif
 
 #ifdef CONFIG_X86_MCE
-idtentry machine_check					has_error_code=0	paranoid=1 do_sym=*machine_check_vector(%rip)
+idtentry machine_check					has_error_code=0	paranoid=1 do_sym="machine_check_vector(%rip)" rap_hash=do_machine_check
 #endif
 
 /*
@@ -1093,11 +1605,11 @@ ENTRY(error_entry)
 	 */
 	TRACE_IRQS_OFF
 	CALL_enter_from_user_mode
-	ret
+	pax_ret error_entry
 
 .Lerror_entry_done:
 	TRACE_IRQS_OFF
-	ret
+	pax_ret error_entry
 
 	/*
 	 * There are two places in the kernel that can potentially fault with
