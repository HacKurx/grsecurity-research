diff --git a/arch/x86/include/asm/alternative-asm.h b/arch/x86/include/asm/alternative-asm.h
index e7636ba..f5c86c4 100644
--- a/arch/x86/include/asm/alternative-asm.h
+++ b/arch/x86/include/asm/alternative-asm.h
@@ -95,6 +205,26 @@
 	.popsection
 .endm
 
+.macro __PAX_REFCOUNT section, counter
+#ifdef CONFIG_PAX_REFCOUNT
+	jo 111f
+	.pushsection .text.\section
+111:	lea \counter,%_ASM_CX
+	int $X86_REFCOUNT_VECTOR
+222:
+	.popsection
+333:
+	_ASM_EXTABLE(222b, 333b)
+#endif
+.endm
+
+.macro PAX_REFCOUNT64_OVERFLOW counter
+	__PAX_REFCOUNT refcount64_overflow, \counter
+.endm
+
+.macro PAX_REFCOUNT64_UNDERFLOW counter
+	__PAX_REFCOUNT refcount64_underflow, \counter
+.endm
 #endif  /*  __ASSEMBLY__  */
 
 #endif /* _ASM_X86_ALTERNATIVE_ASM_H */
diff --git a/arch/x86/include/asm/alternative.h b/arch/x86/include/asm/alternative.h
index 1b02038..d42eaf6 100644
--- a/arch/x86/include/asm/alternative.h
+++ b/arch/x86/include/asm/alternative.h
@@ -238,6 +251,35 @@ static inline int alternatives_text_reserved(void *start, void *end)
  */
 #define ASM_NO_INPUT_CLOBBER(clbr...) "i" (0) : clbr
 
+#ifdef CONFIG_PAX_REFCOUNT
+#define __PAX_REFCOUNT(size)				\
+	"jo 111f\n"					\
+	".if "__stringify(size)" == 4\n\t"		\
+	".pushsection .text.refcount_overflow\n"	\
+	".elseif "__stringify(size)" == -4\n\t"		\
+	".pushsection .text.refcount_underflow\n"	\
+	".elseif "__stringify(size)" == 8\n\t"		\
+	".pushsection .text.refcount64_overflow\n"	\
+	".elseif "__stringify(size)" == -8\n\t"		\
+	".pushsection .text.refcount64_underflow\n"	\
+	".else\n"					\
+	".error \"invalid size\"\n"			\
+	".endif\n"					\
+	"111:\tlea %[counter],%%"_ASM_CX"\n\t"		\
+	"int $"__stringify(X86_REFCOUNT_VECTOR)"\n"	\
+	"222:\n\t"					\
+	".popsection\n"					\
+	"333:\n"					\
+	_ASM_EXTABLE(222b, 333b)
+
+#define PAX_REFCOUNT_OVERFLOW(size)	__PAX_REFCOUNT(size)
+#define PAX_REFCOUNT_UNDERFLOW(size)	__PAX_REFCOUNT(-(size))
+#else
+#define __PAX_REFCOUNT(size)
+#define PAX_REFCOUNT_OVERFLOW(size)
+#define PAX_REFCOUNT_UNDERFLOW(size)
+#endif
+
 #endif /* __ASSEMBLY__ */
 
 #endif /* _ASM_X86_ALTERNATIVE_H */
diff --git a/arch/x86/include/asm/cmpxchg.h b/arch/x86/include/asm/cmpxchg.h
index 97848cd..9ccfae9 100644
--- a/arch/x86/include/asm/cmpxchg.h
+++ b/arch/x86/include/asm/cmpxchg.h
@@ -15,8 +15,12 @@ extern void __cmpxchg_wrong_size(void)
 	__compiletime_error("Bad argument size for cmpxchg");
 extern void __xadd_wrong_size(void)
 	__compiletime_error("Bad argument size for xadd");
+extern void __xadd_check_overflow_wrong_size(void)
+	__compiletime_error("Bad argument size for xadd_check_overflow");
 extern void __add_wrong_size(void)
 	__compiletime_error("Bad argument size for add");
+extern void __add_check_overflow_wrong_size(void)
+	__compiletime_error("Bad argument size for add_check_overflow");
 
 /*
  * Constants for operation sizes. On 32-bit, the 64-bit size it set to
@@ -68,6 +72,32 @@ extern void __add_wrong_size(void)
 		__ret;							\
 	})
 
+#ifdef CONFIG_PAX_REFCOUNT
+#define __xchg_op_check_overflow(ptr, arg, op, lock)			\
+	({								\
+	        __typeof__ (*(ptr)) __ret = (arg);			\
+		switch (sizeof(*(ptr))) {				\
+		case __X86_CASE_L:					\
+			asm volatile (lock #op "l %0, %1\n"		\
+				      PAX_REFCOUNT_OVERFLOW(4)		\
+				      : "+r" (__ret), [counter] "+m" (*(ptr))\
+				      : : "memory", "cc", "cx");	\
+			break;						\
+		case __X86_CASE_Q:					\
+			asm volatile (lock #op "q %q0, %1\n"		\
+				      PAX_REFCOUNT_OVERFLOW(8)		\
+				      : "+r" (__ret), [counter] "+m" (*(ptr))\
+				      : : "memory", "cc", "cx");	\
+			break;						\
+		default:						\
+			__ ## op ## _check_overflow_wrong_size();	\
+		}							\
+		__ret;							\
+	})
+#else
+#define __xchg_op_check_overflow(ptr, arg, op, lock) __xchg_op(ptr, arg, op, lock)
+#endif
+
 /*
  * Note: no "lock" prefix even on SMP: xchg always implies lock anyway.
  * Since this is generally used to protect other memory information, we
@@ -162,6 +192,9 @@ extern void __add_wrong_size(void)
 #define __xadd(ptr, inc, lock)	__xchg_op((ptr), (inc), xadd, lock)
 #define xadd(ptr, inc)		__xadd((ptr), (inc), LOCK_PREFIX)
 
+#define __xadd_check_overflow(ptr, inc, lock)	__xchg_op_check_overflow((ptr), (inc), xadd, lock)
+#define xadd_check_overflow(ptr, inc)		__xadd_check_overflow((ptr), (inc), LOCK_PREFIX)
+
 #define __cmpxchg_double(pfx, p1, p2, o1, o2, n1, n2)			\
 ({									\
 	bool __ret;							\
